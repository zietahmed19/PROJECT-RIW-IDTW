Question ID,Answer ID,Answer Score,Answer Text
231767,231855,17893,"To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.
Iterables
When you create a list, you can read its items one by one. Reading its items one by one is called iteration:
>>> mylist = [1, 2, 3]
>>> for i in mylist:
...    print(i)
1
2
3

mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:
>>> mylist = [x*x for x in range(3)]
>>> for i in mylist:
...    print(i)
0
1
4

Everything you can use ""for... in..."" on is an iterable; lists, strings, files...
These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.
Generators
Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:
>>> mygenerator = (x*x for x in range(3))
>>> for i in mygenerator:
...    print(i)
0
1
4

It is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end after calculating 4, one by one.
Yield
yield is a keyword that is used like return, except the function will return a generator.
>>> def create_generator():
...    mylist = range(3)
...    for i in mylist:
...        yield i*i
...
>>> mygenerator = create_generator() # create a generator
>>> print(mygenerator) # mygenerator is an object!
<generator object create_generator at 0xb7555c34>
>>> for i in mygenerator:
...     print(i)
0
1
4

Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.
To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.
Then, your code will continue from where it left off each time for uses the generator.
Now the hard part:
The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an ""if/else"".

Your code explained
Generator:
# Here you create the method of the node object that will return the generator
def _get_child_candidates(self, distance, min_dist, max_dist):

    # Here is the code that will be called each time you use the generator object:

    # If there is still a child of the node object on its left
    # AND if the distance is ok, return the next child
    if self._leftchild and distance - max_dist < self._median:
        yield self._leftchild

    # If there is still a child of the node object on its right
    # AND if the distance is ok, return the next child
    if self._rightchild and distance + max_dist >= self._median:
        yield self._rightchild

    # If the function arrives here, the generator will be considered empty
    # There are no more than two values: the left and the right children

Caller:
# Create an empty list and a list with the current object reference
result, candidates = list(), [self]

# Loop on candidates (they contain only one element at the beginning)
while candidates:

    # Get the last candidate and remove it from the list
    node = candidates.pop()

    # Get the distance between obj and the candidate
    distance = node._get_dist(obj)

    # If the distance is ok, then you can fill in the result
    if distance <= max_dist and distance >= min_dist:
        result.extend(node._values)

    # Add the children of the candidate to the candidate's list
    # so the loop will keep running until it has looked
    # at all the children of the children of the children, etc. of the candidate
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))

return result

This code contains several smart parts:

The loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.

The extend() method is a list object method that expects an iterable and adds its values to the list.


Usually, we pass a list to it:
>>> a = [1, 2]
>>> b = [3, 4]
>>> a.extend(b)
>>> print(a)
[1, 2, 3, 4]

But in your code, it gets a generator, which is good because:

You don't need to read the values twice.
You may have a lot of children and you don't want them all stored in memory.

And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...
You can stop here, or read a little bit to see an advanced use of a generator:
Controlling a generator exhaustion
>>> class Bank(): # Let's create a bank, building ATMs
...    crisis = False
...    def create_atm(self):
...        while not self.crisis:
...            yield ""$100""
>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want
>>> corner_street_atm = hsbc.create_atm()
>>> print(corner_street_atm.next())
$100
>>> print(corner_street_atm.next())
$100
>>> print([corner_street_atm.next() for cash in range(5)])
['$100', '$100', '$100', '$100', '$100']
>>> hsbc.crisis = True # Crisis is coming, no more money!
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs
>>> print(wall_street_atm.next())
<type 'exceptions.StopIteration'>
>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty
>>> print(corner_street_atm.next())
<type 'exceptions.StopIteration'>
>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business
>>> for cash in brand_new_atm:
...    print cash
$100
$100
$100
$100
$100
$100
$100
$100
$100
...

Note: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))
It can be useful for various things like controlling access to a resource.
Itertools, your best friend
The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?
Chain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list?
Then just import itertools.
An example? Let's see the possible orders of arrival for a four-horse race:
>>> horses = [1, 2, 3, 4]
>>> races = itertools.permutations(horses)
>>> print(races)
<itertools.permutations object at 0xb754f1dc>
>>> print(list(itertools.permutations(horses)))
[(1, 2, 3, 4),
 (1, 2, 4, 3),
 (1, 3, 2, 4),
 (1, 3, 4, 2),
 (1, 4, 2, 3),
 (1, 4, 3, 2),
 (2, 1, 3, 4),
 (2, 1, 4, 3),
 (2, 3, 1, 4),
 (2, 3, 4, 1),
 (2, 4, 1, 3),
 (2, 4, 3, 1),
 (3, 1, 2, 4),
 (3, 1, 4, 2),
 (3, 2, 1, 4),
 (3, 2, 4, 1),
 (3, 4, 1, 2),
 (3, 4, 2, 1),
 (4, 1, 2, 3),
 (4, 1, 3, 2),
 (4, 2, 1, 3),
 (4, 2, 3, 1),
 (4, 3, 1, 2),
 (4, 3, 2, 1)]

Understanding the inner mechanisms of iteration
Iteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).
Iterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.
There is more about it in this article about how for loops work.
"
231767,237028,2528,"Shortcut to understanding yield
When you see a function with yield statements, apply this easy trick to understand what will happen:

Insert a line result = [] at the start of the function.
Replace each yield expr with result.append(expr).
Insert a line return result at the bottom of the function.
Yay - no more yield statements! Read and figure out the code.
Compare the function to the original definition.

This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different than what happens in the list-based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...
Don't confuse your iterables, iterators, and generators
First, the iterator protocol - when you write
for x in mylist:
    ...loop body...

Python performs the following two steps:

Gets an iterator for mylist:
Call iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).
[This is the step most people forget to tell you about]

Uses the iterator to loop over items:
Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.


The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).
Here mylist is an iterable because it implements the iterator protocol. In a user-defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.
So that's the iterator protocol, many objects implement this protocol:

Built-in lists, dictionaries, tuples, sets, and files.
User-defined classes that implement __iter__().
Generators.

Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:
def f123():
    yield 1
    yield 2
    yield 3

for item in f123():
    print item

Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case, a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits.
So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end, however, it runs the function just enough to get the next value out of it and puts it back in suspended mode.
Why use generators?
Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps the state in instance members and performs the next logical step in its next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and prone to bugs. Here generators provide a clean and easy solution.
"
231767,231801,763,"Think of it this way:
An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:
Original version:
def some_function():
    for i in xrange(4):
        yield i

for i in some_function():
    print i

This is basically what the Python interpreter does with the above code:
class it:
    def __init__(self):
        # Start at -1 so that we get 0 when we add 1 below.
        self.count = -1

    # The __iter__ method will be called once by the 'for' loop.
    # The rest of the magic happens on the object returned by this method.
    # In this case it is the object itself.
    def __iter__(self):
        return self

    # The next method will be called repeatedly by the 'for' loop
    # until it raises StopIteration.
    def next(self):
        self.count += 1
        if self.count < 4:
            return self.count
        else:
            # A StopIteration exception is raised
            # to signal that the iterator is done.
            # This is caught implicitly by the 'for' loop.
            raise StopIteration

def some_func():
    return it()

for i in some_func():
    print i

For more insight as to what's happening behind the scenes, the for loop can be rewritten to this:
iterator = some_func()
try:
    while 1:
        print iterator.next()
except StopIteration:
    pass

Does that make more sense or just confuse you more?  :)
I should note that this is an oversimplification for illustrative purposes. :)
"
231767,6400990,655,"
The yield keyword is reduced to two simple facts:

If the compiler detects the yield keyword anywhere inside a function, that function no longer returns via the return statement. Instead, it immediately returns a lazy ""pending list"" object called a generator
A generator is iterable. What is an iterable? It's anything like a list, set, range, dictionary view, or any other object with a built-in protocol for visiting each element in a certain order.

In a nutshell: Most commonly, a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out. Furthermore, advanced usage lets you use generators as coroutines (see below).
generator = myYieldingFunction(...)  # basically a list (but lazy)
x = list(generator)  # evaluate every element into a list

   generator
       v
[x[0], ..., ???]

         generator
             v
[x[0], x[1], ..., ???]

               generator
                   v
[x[0], x[1], x[2], ..., ???]

                       StopIteration exception
[x[0], x[1], x[2]]     done

Basically, whenever the yield statement is encountered, the function pauses and saves its state, then emits ""the next return value in the 'list'"" according to the python iterator protocol (to some syntactic construct like a for-loop that repeatedly calls next() and catches a StopIteration exception, etc.). You might have encountered generators with generator expressions; generator functions are more powerful because you can pass arguments back into the paused generator function, using them to implement coroutines. More on that later.

Basic Example ('list')
Let's define a function makeRange that's just like Python's range. Calling makeRange(n) RETURNS A GENERATOR:
def makeRange(n):
    # return 0,1,2,...,n-1
    i = 0
    while i < n:
        yield i
        i += 1

>>> makeRange(5)
<generator object makeRange at 0x19e4aa0>

To force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable):
>>> list(makeRange(5))
[0, 1, 2, 3, 4]


Comparing the example to ""just returning a list""
The above example can be thought of as merely creating a list that you append to and return:
# return a list                  #  # return a generator
def makeRange(n):                #  def makeRange(n):
    """"""return [0,1,2,...,n-1]"""""" #      """"""return 0,1,2,...,n-1""""""
    TO_RETURN = []               # 
    i = 0                        #      i = 0
    while i < n:                 #      while i < n:
        TO_RETURN += [i]         #          yield i
        i += 1                   #          i += 1
    return TO_RETURN             # 

>>> makeRange(5)
[0, 1, 2, 3, 4]

There is one major difference, though; see the last section.

How you might use generators
An iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so:
#                  < ITERABLE >
>>> [x+10 for x in makeRange(5)]
[10, 11, 12, 13, 14]

To get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop.
Please note: generators can actually be used for many more things, such as implementing coroutines, non-deterministic programming, and other elegant things. However, the ""lazy lists"" viewpoint I present here is the most common use you will find.

Behind the scenes
This is how the ""Python iteration protocol"" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a ""lazy, incremental list"".
>>> x=iter(range(5))
>>> next(x)  # calls x.__next__(); x.next() is deprecated
0
>>> next(x)
1
>>> next(x)
2
>>> next(x)
3
>>> next(x)
4
>>> next(x)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration

The built-in function next() just calls the objects .__next__() function, which is a part of the ""iteration protocol"" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...

Coroutines
Coroutine example:
def interactiveProcedure():
    userResponse = yield makeQuestionWebpage()
    print('user response:', userResponse)
    yield 'success'

coroutine = interactiveProcedure()
webFormData = next(coroutine)  # same as .send(None)
userResponse = serveWebForm(webFormData)

# ...at some point later on web form submit...

successStatus = coroutine.send(userResponse)

A coroutine (generators that generally accept input via the yield keyword e.g. nextInput = yield nextOutput, as a form of two-way communication) is basically a computation that is allowed to pause itself and request input (e.g. to what it should do next). When the coroutine pauses itself (when the running coroutine eventually hits a yield keyword), the computation is paused and control is inverted (yielded) back to the 'calling' function (the frame which requested the next value of the computation). The paused generator/coroutine remains paused until another invoking function (possibly a different function/context) requests the next value to unpause it (usually passing input data to direct the paused logic interior to the coroutine's code).
You can think of Python coroutines as lazy incrementally-pending lists, where the next element doesn't just depend on the previous computation but also on input that you may opt to inject during the generation process.

Minutiae
Normally, most people would not care about the following distinctions and probably want to stop reading here.
In Python-speak, an iterable is any object which ""understands the concept of a for-loop"" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax).
When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.
Thus, in the unlikely event that you are failing to do something like this...
> x = myRange(5)
> list(x)
[0, 1, 2, 3, 4]
> list(x)
[]

... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. If you need to use the result twice, convert the result to a list and store it in a variable x = list(myRange(5)). Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee (still works in Python 3) if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred.
"
231767,31042491,563,"
What does the yield keyword do in Python?

Answer Outline/Summary

A function with yield, when called, returns a Generator.
Generators are iterators because they implement the iterator protocol, so you can iterate over them.
A generator can also be sent information, making it conceptually a coroutine.
In Python 3, you can delegate from one generator to another in both directions with yield from.
(Appendix critiques a couple of answers, including the top one, and discusses the use of return in a generator.)

Generators:
yield is only legal inside of a function definition, and the inclusion of yield in a function definition makes it return a generator.
The idea for generators comes from other languages (see footnote 1) with varying implementations. In Python's Generators, the execution of the code is frozen at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield.
yield provides an
easy way of implementing the iterator protocol, defined by the following two methods:
__iter__ and __next__.  Both of those methods
make an object an iterator that you could type-check with the Iterator Abstract Base
Class from the collections module.
def func():
    yield 'I am'
    yield 'a generator!'

Let's do some introspection:
>>> type(func)                 # A function with yield is still a function
<type 'function'>
>>> gen = func()
>>> type(gen)                  # but it returns a generator
<type 'generator'>
>>> hasattr(gen, '__iter__')   # that's an iterable
True
>>> hasattr(gen, '__next__')   # and with .__next__
True                           # implements the iterator protocol.

The generator type is a sub-type of iterator:
from types import GeneratorType
from collections.abc import Iterator

>>> issubclass(GeneratorType, Iterator)
True

And if necessary, we can type-check like this:
>>> isinstance(gen, GeneratorType)
True
>>> isinstance(gen, Iterator)
True

A feature of an Iterator is that once exhausted, you can't reuse or reset it:
>>> list(gen)
['I am', 'a generator!']
>>> list(gen)
[]

You'll have to make another if you want to use its functionality again (see footnote 2):
>>> list(func())
['I am', 'a generator!']

One can yield data programmatically, for example:
def func(an_iterable):
    for item in an_iterable:
        yield item

The above simple generator is also equivalent to the below - as of Python 3.3 you can use yield from:
def func(an_iterable):
    yield from an_iterable

However, yield from also allows for delegation to subgenerators,
which will be explained in the following section on cooperative delegation with sub-coroutines.
Coroutines:
yield forms an expression that allows data to be sent into the generator (see footnote 3)
Here is an example, take note of the received variable, which will point to the data that is sent to the generator:
def bank_account(deposited, interest_rate):
    while True:
        calculated_interest = interest_rate * deposited 
        received = yield calculated_interest
        if received:
            deposited += received


>>> my_account = bank_account(1000, .05)

First, we must queue up the generator with the built-in function, next. It will
call the appropriate next or __next__ method, depending on the version of
Python you are using:
>>> first_year_interest = next(my_account)
>>> first_year_interest
50.0

And now we can send data into the generator. (Sending None is
the same as calling next.) :
>>> next_year_interest = my_account.send(first_year_interest + 1000)
>>> next_year_interest
102.5

Cooperative Delegation to Sub-Coroutine with yield from
Now, recall that yield from is available in Python 3. This allows us to delegate coroutines to a subcoroutine:

def money_manager(expected_rate):
    # must receive deposited value from .send():
    under_management = yield                   # yield None to start.
    while True:
        try:
            additional_investment = yield expected_rate * under_management 
            if additional_investment:
                under_management += additional_investment
        except GeneratorExit:
            '''TODO: write function to send unclaimed funds to state'''
            raise
        finally:
            '''TODO: write function to mail tax info to client'''
        

def investment_account(deposited, manager):
    '''very simple model of an investment account that delegates to a manager'''
    # must queue up manager:
    next(manager)      # <- same as manager.send(None)
    # This is where we send the initial deposit to the manager:
    manager.send(deposited)
    try:
        yield from manager
    except GeneratorExit:
        return manager.close()  # delegate?

And now we can delegate functionality to a sub-generator and it can be used
by a generator just as above:
my_manager = money_manager(.06)
my_account = investment_account(1000, my_manager)
first_year_return = next(my_account) # -> 60.0

Now simulate adding another 1,000 to the account plus the return on the account (60.0):
next_year_return = my_account.send(first_year_return + 1000)
next_year_return # 123.6

You can read more about the precise semantics of yield from in PEP 380.
Other Methods: close and throw
The close method raises GeneratorExit at the point the function
execution was frozen. This will also be called by __del__ so you
can put any cleanup code where you handle the GeneratorExit:
my_account.close()

You can also throw an exception which can be handled in the generator
or propagated back to the user:
import sys
try:
    raise ValueError
except:
    my_manager.throw(*sys.exc_info())

Raises:
Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""<stdin>"", line 6, in money_manager
  File ""<stdin>"", line 2, in <module>
ValueError

Conclusion
I believe I have covered all aspects of the following question:

What does the yield keyword do in Python?

It turns out that yield does a lot. I'm sure I could add even more
thorough examples to this. If you want more or have some constructive criticism, let me know by commenting
below.

Appendix:
Critique of the Top/Accepted Answer**

It is confused about what makes an iterable, just using a list as an example. See my references above, but in summary: an iterable has an __iter__ method returning an iterator. An iterator additionally provides a .__next__ method, which is implicitly called by for loops until it raises StopIteration, and once it does raise StopIteration, it will continue to do so.
It then uses a generator expression to describe what a generator is. Since a generator expression is simply a convenient way to create an iterator, it only confuses the matter, and we still have not yet gotten to the yield part.
In Controlling a generator exhaustion he calls the .next method (which only works in Python 2), when instead he should use the built-in function, next. Calling next(obj) would be an appropriate layer of indirection, because his code does not work in Python 3.
Itertools? This was not relevant to what yield does at all.
No discussion of the methods that yield provides along with the new functionality yield from in Python 3.

The top/accepted answer is a very incomplete answer.
Critique of answer suggesting yield in a generator expression or comprehension.
The grammar currently allows any expression in a list comprehension.
expr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |
                     ('=' (yield_expr|testlist_star_expr))*)
...
yield_expr: 'yield' [yield_arg]
yield_arg: 'from' test | testlist

Since yield is an expression, it has been touted by some as interesting to use it in comprehensions or generator expression - in spite of citing no particularly good use-case.
The CPython core developers are discussing deprecating its allowance.
Here's a relevant post from the mailing list:

On 30 January 2017 at 19:05, Brett Cannon  wrote:

On Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote:

I'm OK with either approach.  Leaving things the way they are in Python 3
is no good, IMHO.

My vote is it be a SyntaxError since you're not getting what you expect from
the syntax.

I'd agree that's a sensible place for us to end up, as any code
relying on the current behaviour is really too clever to be
maintainable.
In terms of getting there, we'll likely want:

SyntaxWarning or DeprecationWarning in 3.7
Py3k warning in 2.7.x
SyntaxError in 3.8

Cheers, Nick.
--  Nick Coghlan   |   ncoghlan at gmail.com   |   Brisbane, Australia

Further, there is an outstanding issue (10544) which seems to be pointing in the direction of this never being a good idea (PyPy, a Python implementation written in Python, is already raising syntax warnings.)
Bottom line, until the developers of CPython tell us otherwise: Don't put yield in a generator expression or comprehension.
The return statement in a generator
In Python 3:

In a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute.

Historical note, in Python 2:
""In a generator function, the return statement is not allowed to include an expression_list. In that context, a bare return indicates that the generator is done and will cause StopIteration to be raised.""
An expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can't return a value.

Footnotes

The languages CLU, Sather, and Icon were referenced in the proposal
to introduce the concept of generators to Python. The general idea is
that a function can maintain an internal state and yield intermediate
data points on demand by the user. This promised to be superior in performance
to other approaches, including Python threading, which isn't even available on some systems.

 This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects.

 yield was originally introduced as a statement, meaning that it
could only appear at the beginning of a line in a code block.
Now yield creates a yield expression.
https://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt
This change was proposed to allow a user to send data into the generator just as
one might receive it. To send data, one must be able to assign it to something, and
for that, a statement just won't work.


"
419163,419185,8781,"Short Answer
It's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:

If you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.

If you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.


Long Answer
To better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.
Whenever the Python interpreter reads a source file, it does two things:

it sets a few special variables like __name__, and then

it executes all of the code found in the file.


Let's see how this works and how it relates to your question about the __name__ checks we always see in Python scripts.
Code Sample
Let's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py.
# Suppose this is foo.py.

print(""before import"")
import math

print(""before function_a"")
def function_a():
    print(""Function A"")

print(""before function_b"")
def function_b():
    print(""Function B {}"".format(math.sqrt(100)))

print(""before __name__ guard"")
if __name__ == '__main__':
    function_a()
    function_b()
print(""after __name__ guard"")

Special Variables
When the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable.
When Your Module Is the Main Program
If you are running your module (the source file) as the main program, e.g.
python foo.py

the interpreter will assign the hard-coded string ""__main__"" to the __name__ variable, i.e.
# It's as if the interpreter inserts this at the top
# of your module when run as the main program.
__name__ = ""__main__"" 

When Your Module Is Imported By Another
On the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:
# Suppose this is in some other main program.
import foo

The interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name ""foo"" from the import statement to the __name__ variable, i.e.
# It's as if the interpreter inserts this at the top
# of your module when it's imported from another module.
__name__ = ""foo""

Executing the Module's Code
After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.
Always

It prints the string ""before import"" (without quotes).

It loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):


# Find and load a module given its string name, ""math"",
# then assign it to a local variable called math.
math = __import__(""math"")


It prints the string ""before function_a"".

It executes the def block, creating a function object, then assigning that function object to a variable called function_a.

It prints the string ""before function_b"".

It executes the second def block, creating another function object, then assigning it to a variable called function_b.

It prints the string ""before __name__ guard"".


Only When Your Module Is the Main Program

If your module is the main program, then it will see that __name__ was indeed set to ""__main__"" and it calls the two functions, printing the strings ""Function A"" and ""Function B 10.0"".

Only When Your Module Is Imported by Another

(instead) If your module is not the main program but was imported by another one, then __name__ will be ""foo"", not ""__main__"", and it'll skip the body of the if statement.

Always

It will print the string ""after __name__ guard"" in both situations.

Summary
In summary, here's what'd be printed in the two cases:
# What gets printed if foo is the main program
before import
before function_a
before function_b
before __name__ guard
Function A
Function B 10.0
after __name__ guard

# What gets printed if foo is imported as a regular module
before import
before function_a
before function_b
before __name__ guard
after __name__ guard

Why Does It Work This Way?
You might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:

Your module is a library, but you want to have a script mode where it runs some unit tests or a demo.

Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.

Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.


Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. ""Running"" the script is a side effect of importing the script's module.
Food for Thought

Question: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.

Suppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?


# Suppose this is foo2.py.
import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters

def function_a():
    print(""a1"")
    from foo2 import function_b
    print(""a2"")
    function_b()
    print(""a3"")

def function_b():
    print(""b"")

print(""t1"")
if __name__ == ""__main__"":
    print(""m1"")
    function_a()
    print(""m2"")
print(""t2"")
      


Now, figure out what will happen in foo3.py (having removed the __name__ check):

# Suppose this is foo3.py.
import os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters

def function_a():
    print(""a1"")
    from foo3 import function_b
    print(""a2"")
    function_b()
    print(""a3"")

def function_b():
    print(""b"")

print(""t1"")
print(""m1"")
function_a()
print(""m2"")
print(""t2"")


What will this do when used as a script?  When imported as a module?

# Suppose this is in foo4.py
__name__ = ""__main__""

def bar():
    print(""bar"")
    
print(""before __name__ guard"")
if __name__ == ""__main__"":
    bar()
print(""after __name__ guard"")

"
419163,419189,2176,"When your script is run by passing it as a command to the Python interpreter,
python myscript.py

all of the code that is at indentation level 0 gets executed.  Functions and classes that are defined are, well, defined, but none of their code gets run.  Unlike other languages, there's no main() function that gets run automatically - the main() function is implicitly all the code at the top level.
In this case, the top-level code is an if block.  __name__ is a built-in variable which evaluates to the name of the current module.  However, if a module is being run directly (as in myscript.py above), then __name__ instead is set to the string ""__main__"".  Thus, you can test whether your script is being run directly or being imported by something else by testing
if __name__ == ""__main__"":
    ...

If your script is being imported into another module, its various function and class definitions will be imported and its top-level code will be executed, but the code in the then-body of the if clause above won't get run as the condition is not met. As a basic example, consider the following two scripts:
# file one.py
def func():
    print(""func() in one.py"")

print(""top-level in one.py"")

if __name__ == ""__main__"":
    print(""one.py is being run directly"")
else:
    print(""one.py is being imported into another module"")

# file two.py
import one

print(""top-level in two.py"")
one.func()

if __name__ == ""__main__"":
    print(""two.py is being run directly"")
else:
    print(""two.py is being imported into another module"")

Now, if you invoke the interpreter as
python one.py

The output will be
top-level in one.py
one.py is being run directly

If you run two.py instead:
python two.py

You get
top-level in one.py
one.py is being imported into another module
top-level in two.py
func() in one.py
two.py is being run directly

Thus, when module one gets loaded, its __name__ equals ""one"" instead of ""__main__"".
"
419163,419986,884,"Create the following two files:
# a.py

import b

# b.py

print(""__name__ equals "" + __name__)

if __name__ == '__main__':
    print(""if-statement was executed"")

Now run each file individually.

Running python a.py:
$ python a.py
__name__ equals b

When a.py is executed, it imports the module b. This causes all the code inside b to run. Python sets globals()['__name__'] in the b module to the module's name, b.

Running python b.py:
$ python b.py
__name__ equals __main__
if-statement was executed

When only the file b.py is executed, Python sets globals()['__name__'] in this file to ""__main__"". Therefore, the if statement evaluates to True this time.
"
419163,20158605,619,"
What does the if __name__ == ""__main__"": do?

To outline the basics:

The global variable, __name__, in the module that is the entry point to your program, is '__main__'. Otherwise, it's the name you import the module by.
So, code under the if block will only run if the module is the entry point to your program.
It allows the code in the module to be importable by other modules, without executing the code block beneath on import.


Why do we need this?
Developing and Testing Your Code
Say you're writing a Python script designed to be used as a module:
def do_important():
    """"""This function does something very important""""""

You could test the module by adding this call of the function to the bottom:
do_important()

and running it (on a command prompt) with something like:
~$ python important.py

The Problem
However, if you want to import the module to another script:
import important

On import, the do_important function would be called, so you'd probably comment out your function call, do_important(), at the bottom. 
# do_important() # I must remember to uncomment to execute this!

And then you'll have to remember whether or not you've commented out your test function call. And this extra complexity would mean you're likely to forget, making your development process more troublesome.
A Better Way
The __name__ variable points to the namespace wherever the Python interpreter happens to be at the moment. 
Inside an imported module, it's the name of that module. 
But inside the primary module (or an interactive Python session, i.e. the interpreter's Read, Eval, Print Loop, or REPL) you are running everything from its ""__main__"".
So if you check before executing:
if __name__ == ""__main__"":
    do_important()

With the above, your code will only execute when you're running it as the primary module (or intentionally call it from another script). 
An Even Better Way
There's a Pythonic way to improve on this, though. 
What if we want to run this business process from outside the module?
If we put the code we want to exercise as we develop and test in a function like this and then do our check for '__main__' immediately after:
def main():
    """"""business logic for when running this module as the primary one!""""""
    setup()
    foo = do_important()
    bar = do_even_more_important(foo)
    for baz in bar:
        do_super_important(baz)
    teardown()

# Here's our payoff idiom!
if __name__ == '__main__':
    main()

We now have a final function for the end of our module that will run if we run the module as the primary module. 
It will allow the module and its functions and classes to be imported into other scripts without running the main function, and will also allow the module (and its functions and classes) to be called when running from a different '__main__' module, i.e.
import important
important.main()

This idiom can also be found in the Python documentation in an explanation of the __main__ module. That text states:

This module represents the (otherwise anonymous) scope in which the
  interpreter’s main program executes — commands read either from
  standard input, from a script file, or from an interactive prompt. It
  is this environment in which the idiomatic “conditional script” stanza
  causes a script to run:
if __name__ == '__main__':
    main()


"
419163,419174,180,"if __name__ == ""__main__"" is the part that runs when the script is run from (say) the command line using a command like python myscript.py.
"
394809,394814,9056,"Yes, it was added in version 2.5. The expression syntax is:
a if condition else b

First condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.
This allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.
For example:
>>> 'true' if True else 'false'
'true'
>>> 'true' if False else 'false'
'false'

Note that conditionals are an expression, not a statement. This means you can't use statements such as pass, or assignments with = (or ""augmented"" assignments like +=), within a conditional expression:
>>> pass if False else pass
  File ""<stdin>"", line 1
    pass if False else pass
         ^
SyntaxError: invalid syntax

>>> # Python parses this as `x = (1 if False else y) = 2`
>>> # The `(1 if False else x)` part is actually valid, but
>>> # it can't be on the left-hand side of `=`.
>>> x = 1 if False else y = 2
  File ""<stdin>"", line 1
SyntaxError: cannot assign to conditional expression

>>> # If we parenthesize it instead...
>>> (x = 1) if False else (y = 2)
  File ""<stdin>"", line 1
    (x = 1) if False else (y = 2)
       ^
SyntaxError: invalid syntax

(In 3.8 and above, the := ""walrus"" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.)
Similarly, because it is an expression, the else part is mandatory:
# Invalid syntax: we didn't specify what the value should be if the 
# condition isn't met. It doesn't matter if we can verify that
# ahead of time.
a if True

You can, however, use conditional expressions to assign a variable like so:
x = a if True else b

Or for example to return a value:
# Of course we should just use the standard library `max`;
# this is just for demonstration purposes.
def my_max(a, b):
    return a if a > b else b

Think of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal if statement instead.

Keep in mind that it's frowned upon by some Pythonistas for several reasons:

The order of the arguments is different from those of the classic condition ? a : b ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's ""surprising"" behaviour use it (they may reverse the argument order).
Some find it ""unwieldy"", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).
Stylistic reasons. (Although the 'inline if' can be really useful, and make your script more concise, it really does complicate your code)

If you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9.
Official documentation:

Conditional expressions
Is there an equivalent of C’s ”?:” ternary operator?

"
394809,470376,997,"You can index into a tuple:
(falseValue, trueValue)[test]

test needs to return True or False.
It might be safer to always implement it as:
(falseValue, trueValue)[test == True]

or you can use the built-in bool() to assure a Boolean value:
(falseValue, trueValue)[bool(<expression>)]

"
394809,394887,434,"For versions prior to 2.5, there's the trick:
[expression] and [on_true] or [on_false]

It can give wrong results when on_true has a false Boolean value.1
Although it does have the benefit of evaluating expressions left to right, which is clearer in my opinion.
1. Is there an equivalent of C’s ”?:” ternary operator?
"
394809,2919360,355," <expression 1> if <condition> else <expression 2> 
a = 1
b = 2

1 if a > b else -1 
# Output is -1

1 if a > b else -1 if a < b else 0
# Output is -1

"
394809,394815,206,"From the documentation:

Conditional expressions (sometimes called a “ternary operator”) have the lowest priority of all Python operations.
The expression x if C else y first evaluates the condition, C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.
See PEP 308 for more details about conditional expressions.

New since version 2.5.
"
100003,6581949,8954,"Classes as objects
Prior to delving into metaclasses, a solid grasp of Python classes is beneficial. Python holds a particularly distinctive concept of classes, a notion it adopts from the Smalltalk language.
In most languages, classes are just pieces of code that describe how to produce an object. That is somewhat true in Python too:
>>> class ObjectCreator(object):
...     pass

>>> my_object = ObjectCreator()
>>> print(my_object)
    <__main__.ObjectCreator object at 0x8974f2c>

But classes are more than that in Python. Classes are objects too.
Yes, objects.
When a Python script runs, every line of code is executed from top to bottom. When the Python interpreter encounters the class keyword, Python creates an object out of the ""description"" of the class that follows. Thus, the following instruction
>>> class ObjectCreator(object):
...     pass

...creates an object with the name ObjectCreator!
This object (the class) is itself capable of creating objects (called instances).
But still, it's an object. Therefore, like all objects:

you can assign it to a variable1
JustAnotherVariable = ObjectCreator


you can attach attributes to it
ObjectCreator.class_attribute = 'foo'


you can pass it as a function parameter
print(ObjectCreator)



1 Note that merely assigning it to another variable doesn't change the class's __name__, i.e.,
>>> print(JustAnotherVariable)
    <class '__main__.ObjectCreator'>

>>> print(JustAnotherVariable())
    <__main__.ObjectCreator object at 0x8997b4c>


Creating classes dynamically
Since classes are objects, you can create them on the fly, like any object.
First, you can create a class in a function using class:
>>> def choose_class(name):
...     if name == 'foo':
...         class Foo(object):
...             pass
...         return Foo # return the class, not an instance
...     else:
...         class Bar(object):
...             pass
...         return Bar

>>> MyClass = choose_class('foo')

>>> print(MyClass) # the function returns a class, not an instance
    <class '__main__.Foo'>

>>> print(MyClass()) # you can create an object from this class
    <__main__.Foo object at 0x89c6d4c>

But it's not so dynamic, since you still have to write the whole class yourself.
Since classes are objects, they must be generated by something.
When you use the class keyword, Python creates this object automatically. But as
with most things in Python, it gives you a way to do it manually.
Remember the function type? The good old function that lets you know what
type an object is:
>>> print(type(1))
    <type 'int'>

>>> print(type(""1""))
    <type 'str'>

>>> print(type(ObjectCreator))
    <type 'type'>

>>> print(type(ObjectCreator()))
    <class '__main__.ObjectCreator'>

Well, type has also a completely different ability: it can create classes on the fly. type can take the description of a class as parameters,
and return a class.
(I  know, it's silly that the same function can have two completely different uses according to the parameters you pass to it. It's an issue due to backward
compatibility in Python)
type works this way:
type(name, bases, attrs)

Where:

name: name of the class
bases: tuple of the parent class (for inheritance, can be empty)
attrs: dictionary containing attributes names and values

e.g.:
>>> class MyShinyClass(object):
...     pass

can be created manually this way:
>>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object
>>> print(MyShinyClass)
    <class '__main__.MyShinyClass'>

>>> print(MyShinyClass()) # create an instance with the class
    <__main__.MyShinyClass object at 0x8997cec>

You'll notice that we use MyShinyClass as the name of the class
and as the variable to hold the class reference. They can be different,
but there is no reason to complicate things.
type accepts a dictionary to define the attributes of the class. So:
>>> class Foo(object):
...     bar = True

Can be translated to:
>>> Foo = type('Foo', (), {'bar':True})

And used as a normal class:
>>> print(Foo)
    <class '__main__.Foo'>

>>> print(Foo.bar)
    True

>>> f = Foo()
>>> print(f)
    <__main__.Foo object at 0x8a9b84c>

>>> print(f.bar)
    True

And of course, you can inherit from it, so:
>>> class FooChild(Foo):
...     pass

would be:
>>> FooChild = type('FooChild', (Foo,), {})
>>> print(FooChild)
    <class '__main__.FooChild'>

>>> print(FooChild.bar) # bar is inherited from Foo
    True

Eventually, you'll want to add methods to your class. Just define a function
with the proper signature and assign it as an attribute.
>>> def echo_bar(self):
...     print(self.bar)

>>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})

>>> hasattr(Foo, 'echo_bar')
    False

>>> hasattr(FooChild, 'echo_bar')
    True

>>> my_foo = FooChild()
>>> my_foo.echo_bar()
    True

And you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object.
>>> def echo_bar_more(self):
...     print('yet another method')

>>> FooChild.echo_bar_more = echo_bar_more
>>> hasattr(FooChild, 'echo_bar_more')
    True

You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.
This is what Python does when you use the keyword class, and it does so by using a metaclass.
What are metaclasses (finally)
Metaclasses are the 'stuff' that creates classes.
You define classes in order to create objects, right?
But we learned that Python classes are objects.
Well, metaclasses are what create these objects. They are the classes' classes,
you can picture them this way:
MyClass = MetaClass()
my_object = MyClass()

You've seen that type lets you do something like this:
MyClass = type('MyClass', (), {})

It's because the function type is in fact a metaclass. type is the
metaclass Python uses to create all classes behind the scenes.
Now you wonder ""why the heck is it written in lowercase, and not Type?""
Well, I guess it's a matter of consistency with str, the class that creates
strings objects, and int the class that creates integer objects. type is
just the class that creates class objects.
You see that by checking the __class__ attribute.
Everything, and I mean everything, is an object in Python. That includes integers,
strings, functions and classes. All of them are objects. And all of them have
been created from a class:
>>> age = 35
>>> age.__class__
    <type 'int'>

>>> name = 'bob'
>>> name.__class__
    <type 'str'>

>>> def foo(): pass
>>> foo.__class__
    <type 'function'>

>>> class Bar(object): pass
>>> b = Bar()
>>> b.__class__
    <class '__main__.Bar'>

Now, what is the __class__ of any __class__ ?
>>> age.__class__.__class__
    <type 'type'>

>>> name.__class__.__class__
    <type 'type'>

>>> foo.__class__.__class__
    <type 'type'>

>>> b.__class__.__class__
    <type 'type'>

So, a metaclass is just the stuff that creates class objects.
You can call it a 'class factory' if you wish.
type is the built-in metaclass Python uses, but of course, you can create your
own metaclass.
The __metaclass__ attribute
In Python 2, you can add a __metaclass__ attribute when you write a class (see next section for the Python 3 syntax):
class Foo(object):
    __metaclass__ = something...
    [...]

If you do so, Python will use the metaclass to create the class Foo.
Careful, it's tricky.
You write class Foo(object) first, but the class object Foo is not created
in memory yet.
Python will look for __metaclass__ in the class definition. If it finds it,
it will use it to create the object class Foo. If it doesn't, it will use
type to create the class.
Read that several times.
When you do:
class Foo(Bar):
    pass

Python does the following:
Is there a __metaclass__ attribute in Foo?
If yes, create in-memory a class object (I said a class object, stay with me here), with the name Foo by using what is in __metaclass__.
If Python can't find __metaclass__, it will look for a __metaclass__ at the MODULE level, and try to do the same (but only for classes that don't inherit anything, basically old-style classes).
Then if it can't find any __metaclass__ at all, it will use the Bar's (the first parent) own metaclass (which might be the default type) to create the class object.
Be careful here that the __metaclass__ attribute will not be inherited, the metaclass of the parent (Bar.__class__) will be. If Bar used a __metaclass__ attribute that created Bar with type() (and not type.__new__()), the subclasses will not inherit that behavior.
Now the big question is, what can you put in __metaclass__?
The answer is something that can create a class.
And what can create a class? type, or anything that subclasses or uses it.
Metaclasses in Python 3
The syntax to set the metaclass has been changed in Python 3:
class Foo(object, metaclass=something):
    ...

i.e. the __metaclass__ attribute is no longer used, in favor of a keyword argument in the list of base classes.
The behavior of metaclasses however stays largely the same.
One thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so:
class Foo(object, metaclass=something, kwarg1=value1, kwarg2=value2):
    ...

Read the section below for how Python handles this.
Custom metaclasses
The main purpose of a metaclass is to change the class automatically,
when it's created.
You usually do this for APIs, where you want to create classes matching the
current context.
Imagine a stupid example, where you decide that all classes in your module
should have their attributes written in uppercase. There are several ways to
do this, but one way is to set __metaclass__ at the module level.
This way, all classes of this module will be created using this metaclass,
and we just have to tell the metaclass to turn all attributes to uppercase.
Luckily, __metaclass__ can actually be any callable, it doesn't need to be a
formal class (I know, something with 'class' in its name doesn't need to be
a class, go figure... but it's helpful).
So we will start with a simple example, by using a function.
# the metaclass will automatically get passed the same argument
# that you usually pass to `type`
def upper_attr(future_class_name, future_class_parents, future_class_attrs):
    """"""
      Return a class object, with the list of its attribute turned
      into uppercase.
    """"""
    # pick up any attribute that doesn't start with '__' and uppercase it
    uppercase_attrs = {
        attr if attr.startswith(""__"") else attr.upper(): v
        for attr, v in future_class_attrs.items()
    }

    # let `type` do the class creation
    return type(future_class_name, future_class_parents, uppercase_attrs)

__metaclass__ = upper_attr # this will affect all classes in the module

class Foo(): # global __metaclass__ won't work with ""object"" though
    # but we can define __metaclass__ here instead to affect only this class
    # and this will work with ""object"" children
    bar = 'bip'

Let's check:
>>> hasattr(Foo, 'bar')
    False

>>> hasattr(Foo, 'BAR')
    True

>>> Foo.BAR
    'bip'

Now, let's do exactly the same, but using a real class for a metaclass:
# remember that `type` is actually a class like `str` and `int`
# so you can inherit from it
class UpperAttrMetaclass(type):
    # __new__ is the method called before __init__
    # it's the method that creates the object and returns it
    # while __init__ just initializes the object passed as parameter
    # you rarely use __new__, except when you want to control how the object
    # is created.
    # here the created object is the class, and we want to customize it
    # so we override __new__
    # you can do some stuff in __init__ too if you wish
    # some advanced use involves overriding __call__ as well, but we won't
    # see this
    def __new__(
        upperattr_metaclass,
        future_class_name,
        future_class_parents,
        future_class_attrs
    ):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in future_class_attrs.items()
        }
        return type(future_class_name, future_class_parents, uppercase_attrs)

Let's rewrite the above, but with shorter and more realistic variable names now that we know what they mean:
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }
        return type(clsname, bases, uppercase_attrs)

You may have noticed the extra argument cls. There is
nothing special about it: __new__ always receives the class it's defined in, as the first parameter. Just like you have self for ordinary methods which receive the instance as the first parameter, or the defining class for class methods.
But this is not proper OOP. We are calling type directly and we aren't overriding or calling the parent's __new__. Let's do that instead:
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }
        return type.__new__(cls, clsname, bases, uppercase_attrs)

We can make it even cleaner by using super, which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):
class UpperAttrMetaclass(type):
    def __new__(cls, clsname, bases, attrs):
        uppercase_attrs = {
            attr if attr.startswith(""__"") else attr.upper(): v
            for attr, v in attrs.items()
        }

        # Python 2 requires passing arguments to super:
        return super(UpperAttrMetaclass, cls).__new__(
            cls, clsname, bases, uppercase_attrs)

        # Python 3 can use no-arg super() which infers them:
        return super().__new__(cls, clsname, bases, uppercase_attrs)

Oh, and in Python 3 if you do this call with keyword arguments, like this:
class Foo(object, metaclass=MyMetaclass, kwarg1=value1):
    ...

It translates to this in the metaclass to use it:
class MyMetaclass(type):
    def __new__(cls, clsname, bases, dct, kwargs1=default):
        ...

That's it. There is really nothing more about metaclasses.
The reason behind the complexity of the code using metaclasses is not because
of metaclasses, it's because you usually use metaclasses to do twisted stuff
relying on introspection, manipulating inheritance, vars such as __dict__, etc.
Indeed, metaclasses are especially useful to do black magic, and therefore
complicated stuff. But by themselves, they are simple:

intercept a class creation
modify the class
return the modified class

Why would you use metaclasses classes instead of functions?
Since __metaclass__ can accept any callable, why would you use a class
since it's obviously more complicated?
There are several reasons to do so:

The intention is clear. When you read UpperAttrMetaclass(type), you know
what's going to follow
You can use OOP. Metaclass can inherit from metaclass, override parent methods. Metaclasses can even use metaclasses.
Subclasses of a class will be instances of its metaclass if you specified a metaclass-class, but not with a metaclass-function.
You can structure your code better. You never use metaclasses for something as trivial as the above example. It's usually for something complicated. Having the ability to make several methods and group them in one class is very useful to make the code easier to read.
You can hook on __new__, __init__ and __call__. Which will allow you to do different stuff, Even if usually you can do it all in __new__,
some people are just more comfortable using __init__.
These are called metaclasses, damn it! It must mean something!

Why would you use metaclasses?
Now the big question. Why would you use some obscure error-prone feature?
Well, usually you don't:

Metaclasses are deeper magic that
99% of users should never worry about it.
If you wonder whether you need them,
you don't (the people who actually
need them know with certainty that
they need them, and don't need an
explanation about why).

Python Guru Tim Peters
The main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this:
class Person(models.Model):
    name = models.CharField(max_length=30)
    age = models.IntegerField()

But if you do this:
person = Person(name='bob', age='35')
print(person.age)

It won't return an IntegerField object. It will return an int, and can even take it directly from the database.
This is possible because models.Model defines __metaclass__ and
it uses some magic that will turn the Person you just defined with simple statements
into a complex hook to a database field.
Django makes something complex look simple by exposing a simple API
and using metaclasses, recreating code from this API to do the real job
behind the scenes.
The last word
First, you know that classes are objects that can create instances.
Well, in fact, classes are themselves instances. Of metaclasses.
>>> class Foo(object): pass
>>> id(Foo)
    142630324

Everything is an object in Python, and they are all either instance of classes
or instances of metaclasses.
Except for type.
type is actually its own metaclass. This is not something you could
reproduce in pure Python, and is done by cheating a little bit at the implementation
level.
Secondly, metaclasses are complicated. You may not want to use them for
very simple class alterations. You can change classes by using two different techniques:

monkey patching
class decorators

99% of the time you need class alteration, you are better off using these.
But 98% of the time, you don't need class alteration at all.
"
100003,100146,3426,"A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.
While in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.
A metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.
When the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.
However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.
Here's an aggregated example of the bits and pieces:
def make_hook(f):
    """"""Decorator to turn 'foo' method into '__foo__'""""""
    f.is_hook = 1
    return f

class MyType(type):
    def __new__(mcls, name, bases, attrs):

        if name.startswith('None'):
            return None

        # Go over attributes and see if they should be renamed.
        newattrs = {}
        for attrname, attrvalue in attrs.iteritems():
            if getattr(attrvalue, 'is_hook', 0):
                newattrs['__%s__' % attrname] = attrvalue
            else:
                newattrs[attrname] = attrvalue

        return super(MyType, mcls).__new__(mcls, name, bases, newattrs)

    def __init__(self, name, bases, attrs):
        super(MyType, self).__init__(name, bases, attrs)

        # classregistry.register(self, self.interfaces)
        print ""Would register class %s now."" % self

    def __add__(self, other):
        class AutoClass(self, other):
            pass
        return AutoClass
        # Alternatively, to autogenerate the classname as well as the class:
        # return type(self.__name__ + other.__name__, (self, other), {})

    def unregister(self):
        # classregistry.unregister(self)
        print ""Would unregister class %s now."" % self

class MyObject:
    __metaclass__ = MyType


class NoneSample(MyObject):
    pass

# Will print ""NoneType None""
print type(NoneSample), repr(NoneSample)

class Example(MyObject):
    def __init__(self, value):
        self.value = value
    @make_hook
    def add(self, other):
        return self.__class__(self.value + other.value)

# Will unregister the class
Example.unregister()

inst = Example(10)
# Will fail with an AttributeError
#inst.unregister()

print inst + inst
class Sibling(MyObject):
    pass

ExampleSibling = Example + Sibling
# ExampleSibling is now a subclass of both Example and Sibling (with no
# content of its own) although it will believe it's called 'AutoClass'
print ExampleSibling
print ExampleSibling.__mro__

"
100003,100037,490,"Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x.
Metaclasses are the secret sauce that make 'class' work. The default metaclass for a new style object is called 'type'.
class type(object)
  |  type(object) -> the object's type
  |  type(name, bases, dict) -> a new type

Metaclasses take 3 args. 'name', 'bases' and 'dict'
Here is where the secret starts. Look for where name, bases and the dict come from in this example class definition.
class ThisIsTheName(Bases, Are, Here):
    All_the_code_here
    def doesIs(create, a):
        dict

Lets define a metaclass that will demonstrate how 'class:' calls it.
def test_metaclass(name, bases, dict):
    print 'The Class Name is', name
    print 'The Class Bases are', bases
    print 'The dict has', len(dict), 'elems, the keys are', dict.keys()

    return ""yellow""

class TestName(object, None, int, 1):
    __metaclass__ = test_metaclass
    foo = 1
    def baz(self, arr):
        pass

print 'TestName = ', repr(TestName)

# output => 
The Class Name is TestName
The Class Bases are (<type 'object'>, None, <type 'int'>, 1)
The dict has 4 elems, the keys are ['baz', '__module__', 'foo', '__metaclass__']
TestName =  'yellow'

And now, an example that actually means something, this will automatically make the variables in the list ""attributes"" set on the class, and set to None.
def init_attributes(name, bases, dict):
    if 'attributes' in dict:
        for attr in dict['attributes']:
            dict[attr] = None

    return type(name, bases, dict)

class Initialised(object):
    __metaclass__ = init_attributes
    attributes = ['foo', 'bar', 'baz']

print 'foo =>', Initialised.foo
# output=>
foo => None

Note that the magic behaviour that Initialised gains by having the metaclass init_attributes is not passed onto a subclass of Initialised.
Here is an even more concrete example, showing how you can subclass 'type' to make a metaclass that performs an action when the class is created. This is quite tricky:
class MetaSingleton(type):
    instance = None
    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(MetaSingleton, cls).__call__(*args, **kw)
        return cls.instance

class Foo(object):
    __metaclass__ = MetaSingleton

a = Foo()
b = Foo()
assert a is b

"
100003,6428779,230,"Others have explained how metaclasses work and how they fit into the Python type system. Here's an example of what they can be used for. In a testing framework I wrote, I wanted to keep track of the order in which classes were defined, so that I could later instantiate them in this order. I found it easiest to do this using a metaclass.
class MyMeta(type):

    counter = 0

    def __init__(cls, name, bases, dic):
        type.__init__(cls, name, bases, dic)
        cls._order = MyMeta.counter
        MyMeta.counter += 1

class MyType(object):              # Python 2
    __metaclass__ = MyMeta

class MyType(metaclass=MyMeta):    # Python 3
    pass

Anything that's a subclass of MyType then gets a class attribute _order that records the order in which the classes were defined.
"
100003,100091,200,"One use for metaclasses is adding new properties and methods to an instance automatically.
For example, if you look at Django models, their definition looks a bit confusing. It looks as if you are only defining class properties:
class Person(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=30)

However, at runtime the Person objects are filled with all sorts of useful methods. See the source for some amazing metaclassery.
"
82831,82852,6484,"If the reason you're checking is so you can do something like if file_exists: open_it(), it's safer to use a try around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.
If you're not planning to open the file immediately, you can use os.path.isfile

Return True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.

import os.path
os.path.isfile(fname)

if you need to be sure it's a file.
Starting with Python 3.4, the pathlib module offers an object-oriented approach (backported to pathlib2 in Python 2.7):
from pathlib import Path

my_file = Path(""/path/to/file"")
if my_file.is_file():
    # file exists

To check a directory, do:
if my_file.is_dir():
    # directory exists

To check whether a Path object exists independently of whether is it a file or directory, use exists():
if my_file.exists():
    # path exists

You can also use resolve(strict=True) in a try block:
try:
    my_abs_path = my_file.resolve(strict=True)
except FileNotFoundError:
    # doesn't exist
else:
    # exists

"
82831,82846,2601,"Use os.path.exists to check both files and directories:
import os.path
os.path.exists(file_path)

Use os.path.isfile to check only files (note: follows symbolic links):
os.path.isfile(file_path)

"
82831,84173,1224,"Unlike isfile(), exists() will return True for directories. So depending on if you want only plain files or also directories, you'll use isfile() or exists(). Here is some simple REPL output:
>>> os.path.isfile(""/etc/password.txt"")
True
>>> os.path.isfile(""/etc"")
False
>>> os.path.isfile(""/does/not/exist"")
False
>>> os.path.exists(""/etc/password.txt"")
True
>>> os.path.exists(""/etc"")
True
>>> os.path.exists(""/does/not/exist"")
False

"
82831,82836,880,"import os

if os.path.isfile(filepath):
   print(""File exists"")

"
82831,8876254,426,"Use os.path.isfile() with os.access():
import os

PATH = './file.txt'
if os.path.isfile(PATH) and os.access(PATH, os.R_OK):
    print(""File exists and is readable"")
else:
    print(""Either the file is missing or not readable"")

"
5767325,5767357,16496,"Find the index of the array element you want to remove using indexOf, and then remove that index with splice.

The splice() method changes the contents of an array by removing
existing elements and/or adding new elements.



const array = [2, 5, 9];

console.log(array);

const index = array.indexOf(5);
if (index > -1) { // only splice array when item is found
  array.splice(index, 1); // 2nd parameter means remove one item only
}

// array = [2, 9]
console.log(array); 



The second parameter of splice is the number of elements to remove. Note that splice modifies the array in place and returns a new array containing the elements that have been removed.

For the reason of completeness, here are functions. The first function removes only a single occurrence (i.e. removing the first match of 5 from [2,5,9,1,5,8,5]), while the second function removes all occurrences:


function removeItemOnce(arr, value) {
  var index = arr.indexOf(value);
  if (index > -1) {
    arr.splice(index, 1);
  }
  return arr;
}

function removeItemAll(arr, value) {
  var i = 0;
  while (i < arr.length) {
    if (arr[i] === value) {
      arr.splice(i, 1);
    } else {
      ++i;
    }
  }
  return arr;
}
// Usage
console.log(removeItemOnce([2,5,9,1,5,8,5], 5))
console.log(removeItemAll([2,5,9,1,5,8,5], 5))



In TypeScript, these functions can stay type-safe with a type parameter:
function removeItem<T>(arr: Array<T>, value: T): Array<T> { 
  const index = arr.indexOf(value);
  if (index > -1) {
    arr.splice(index, 1);
  }
  return arr;
}

"
5767325,20690490,2438,"Edited on 2016 October

Do it simple, intuitive and explicit (Occam's razor)
Do it immutable (original array stays unchanged)
Do it with standard JavaScript functions, if your browser doesn't support them - use polyfill

In this code example I use array.filter(...) function to remove unwanted items from an array. This function doesn't change the original array and creates a new one. If your browser doesn't support this function (e.g. Internet Explorer before version 9, or Firefox before version 1.5), consider polyfilling with core-js.
Be mindful though, creating a new array every time takes a big performance hit. If the list is very large (think 10k+ items) then consider using other methods.
Removing item (ECMA-262 Edition 5 code AKA old style JavaScript)


var value = 3

var arr = [1, 2, 3, 4, 5, 3]

arr = arr.filter(function(item) {
    return item !== value
})

console.log(arr)
// [ 1, 2, 4, 5 ]



Removing item (ECMAScript 6 code)


let value = 3

let arr = [1, 2, 3, 4, 5, 3]

arr = arr.filter(item => item !== value)

console.log(arr)
// [ 1, 2, 4, 5 ]



IMPORTANT ECMAScript 6 () => {} arrow function syntax is not supported in Internet Explorer at all, Chrome before version 45, Firefox before version 22, and Safari before version 10. To use ECMAScript 6 syntax in old browsers you can use BabelJS.

Removing multiple items (ECMAScript 7 code)
An additional advantage of this method is that you can remove multiple items


let forDeletion = [2, 3, 5]

let arr = [1, 2, 3, 4, 5, 3]

arr = arr.filter(item => !forDeletion.includes(item))
// !!! Read below about array.includes(...) support !!!

console.log(arr)
// [ 1, 4 ]



IMPORTANT array.includes(...) function is not supported in Internet Explorer at all, Chrome before version 47, Firefox before version 43, Safari before version 9, and Edge before version 14 but you can polyfill with core-js.
Removing multiple items (in the future, maybe)
If the ""This-Binding Syntax"" proposal is ever accepted, you'll be able to do this:


// array-lib.js

export function remove(...forDeletion) {
    return this.filter(item => !forDeletion.includes(item))
}

// main.js

import { remove } from './array-lib.js'

let arr = [1, 2, 3, 4, 5, 3]

// :: This-Binding Syntax Proposal
// using ""remove"" function as ""virtual method""
// without extending Array.prototype
arr = arr::remove(2, 3, 5)

console.log(arr)
// [ 1, 4 ]



Try it yourself in BabelJS :)
Reference

Array.prototype.includes
Functional composition

"
5767325,5767335,1749,"I don't know how you are expecting array.remove(int) to behave. There are three possibilities I can think of that you might want.
To remove an element of an array at an index i:
array.splice(i, 1);

If you want to remove every element with value number from the array:
for (var i = array.length - 1; i >= 0; i--) {
 if (array[i] === number) {
  array.splice(i, 1);
 }
}

If you just want to make the element at index i no longer exist, but you don't want the indexes of the other elements to change:
delete array[i];

"
5767325,5767411,666,"It depends on whether you want to keep an empty spot or not.
If you do want an empty slot:
array[index] = undefined;

If you don't want an empty slot:
//To keep the original:
//oldArray = [...array];

//This modifies the array.
array.splice(index, 1);

And if you need the value of that item, you can just store the returned array's element:
var value = array.splice(index, 1)[0];

If you want to remove at either end of the array, you can use array.pop() for the last one or array.shift() for the first one (both return the value of the item as well).
If you don't know the index of the item, you can use array.indexOf(item) to get it (in a if() to get one item or in a while() to get all of them). array.indexOf(item) returns either the index or -1 if not found. 
"
5767325,18165553,418,"A friend was having issues in Internet Explorer 8 and showed me what he did. I told him it was wrong, and he told me he got the answer here. The current top answer will not work in all browsers (Internet Explorer 8 for example), and it will only remove the first occurrence of the item.
Remove ALL instances from an array
function removeAllInstances(arr, item) {
   for (var i = arr.length; i--;) {
     if (arr[i] === item) arr.splice(i, 1);
   }
}

It loops through the array backwards (since indices and length will change as items are removed) and removes the item if it's found. It works in all browsers.
"
178325,178450,10231,"Since the question refers to a single element, this code might be more suitable:
// Checks CSS content for display:[none|block], ignores visibility:[true|false]
$(element).is("":visible"");

// The same works with hidden
$(element).is("":hidden"");

It is the same as twernt's suggestion, but applied to a single element; and it matches the algorithm recommended in the jQuery FAQ.
We use jQuery's is() to check the selected element with another element, selector or any jQuery object. This method traverses along the DOM elements to find a match, which satisfies the passed parameter. It will return true if there is a match, otherwise return false.
"
178325,178386,1622,"You can use the hidden selector:
// Matches all elements that are hidden
$('element:hidden')

And the visible selector:
// Matches all elements that are visible
$('element:visible')

"
178325,178344,1097,"if ( $(element).css('display') == 'none' || $(element).css(""visibility"") == ""hidden""){
    // 'element' is hidden
}

The above method does not consider the visibility of the parent. To consider the parent as well, you should use .is("":hidden"") or .is("":visible"").
For example,
<div id=""div1"" style=""display:none"">
  <div id=""div2"" style=""display:block"">Div2</div>
</div>


The above method will consider div2 visible while :visible not. But the above might be useful in many cases, especially when you need to find if there is any error divs visible in the hidden parent because in such conditions :visible will not work.

"
178325,5423934,593,"None of these answers address what I understand to be the question, which is what I was searching for, ""How do I handle items that have visibility: hidden?"". Neither :visible nor :hidden will handle this, as they are both looking for display per the documentation.  As far as I could determine, there is no selector to handle CSS visibility.  Here is how I resolved it (standard jQuery selectors, there may be a more condensed syntax):
$("".item"").each(function() {
    if ($(this).css(""visibility"") == ""hidden"") {
        // handle non visible state
    } else {
        // handle visible state
    }
});

"
178325,4685330,434,"From How do I determine the state of a toggled element?

You can determine whether an element is collapsed or not by using the :visible and :hidden selectors.
var isVisible = $('#myDiv').is(':visible');
var isHidden = $('#myDiv').is(':hidden');

If you're simply acting on an element based on its visibility, you can just include :visible or :hidden in the selector expression. For example:
 $('#myDiv:visible').animate({left: '+=200px'}, 'slow');

"
1335851,1335881,5483,"Update for ES6 modules
Inside native ECMAScript modules (with import and export statements) and ES6 classes, strict mode is always enabled and cannot be disabled.
Original answer
This article about Javascript Strict Mode might interest you: John Resig - ECMAScript 5 Strict Mode, JSON, and More
To quote some interesting parts:

Strict Mode is a new feature in ECMAScript 5 that allows you to place a program, or a function, in a ""strict"" operating context. This strict context prevents certain actions from being taken and throws more exceptions.

And:

Strict mode helps out in a couple ways:

It catches some common coding bloopers, throwing exceptions.
It prevents, or throws errors, when relatively ""unsafe"" actions are taken (such as gaining access to the global object).
It disables features that are confusing or poorly thought out.


Also note you can apply ""strict mode"" to the whole file... Or you can use it only for a specific function (still quoting from John Resig's article):
// Non-strict code...

(function(){
  ""use strict"";

  // Define your library strictly...
})();

// Non-strict code...

Which might be helpful if you have to mix old and new code ;-)
So, I suppose it's a bit like the ""use strict"" you can use in Perl (hence the name?): it helps you make fewer errors, by detecting more things that could lead to breakages.
Strict mode is now supported by all major browsers.
"
1335851,1335875,1427,"It's a new feature of ECMAScript 5. John Resig wrote up a nice summary of it.
It's just a string you put in your JavaScript files (either at the top of your file or inside of a function) that looks like this:
""use strict"";

Putting it in your code now shouldn't cause any problems with current browsers as it's just a string. It may cause problems with your code in the future if your code violates the pragma.  For instance, if you currently have foo = ""bar"" without defining foo first, your code will start failing...which is a good thing in my opinion.
"
1335851,27114400,858,"The statement ""use strict"";  instructs the browser to use the Strict mode, which is a reduced and safer feature set of JavaScript.
List of features (non-exhaustive)

Disallows global variables. (Catches missing var declarations and typos in variable names)

Silent failing assignments will throw error in strict mode (assigning NaN = 5;)

Attempts to delete undeletable properties will throw (delete Object.prototype)

Requires all property names in an object literal to be unique (var x = {x1: ""1"", x1: ""2""})

Function parameter names must be unique (function sum (x, x) {...})

Forbids octal syntax (var x = 023; some devs assume wrongly that a preceding zero does nothing to change the number.)

Forbids the with keyword

eval in strict mode does not introduce new variables

Forbids deleting plain names (delete x;)

Forbids binding or assignment of the names eval and arguments in any form

Strict mode does not alias properties of the arguments object with the formal parameters. (e.g. in function sum (a,b) { return arguments[0] + b;} This works because arguments[0] is bound to a and so on. ) (See examples section below to understand the difference)

arguments.callee is not supported


[Ref: Strict mode, Mozilla Developer Network]

Examples:

Strict mode code doesn't alias properties of arguments objects created within it

function show( msg ){
    msg = 42;
    console.log( msg );          // msg === 42
    console.log( arguments[0] ); // arguments === 42
}
show( ""Hey"" );

// In strict mode arguments[i] does not track the value of 
// the corresponding named argument, nor does a named argument track the value in the corresponding arguments[i]
function showStrict( msg ){
    ""use strict"";
    msg = 42;
    console.log( msg );          // msg === 42
    console.log( arguments[0] ); // arguments === ""Hey""
}
showStrict( ""Hey"" );

"
1335851,11496488,486,"If people are worried about using use strict it might be worth checking out this article: 
ECMAScript 5 'Strict mode' support in browsers. What does this mean?
NovoGeek.com - Krishna's weblog
It talks about browser support, but more importantly how to deal with it safely:
function isStrictMode(){
    return !this;
} 
/*
   returns false, since 'this' refers to global object and 
   '!this' becomes false
*/

function isStrictMode(){   
    ""use strict"";
    return !this;
} 
/* 
   returns true, since in strict mode the keyword 'this'
   does not refer to global object, unlike traditional JS. 
   So here, 'this' is 'undefined' and '!this' becomes true.
*/

"
1335851,22141126,264,"A word of caution, all you hard-charging programmers:  applying ""use strict"" to existing code can be hazardous!  This thing is not some feel-good, happy-face sticker that you can slap on the code to make it 'better'.  With the ""use strict"" pragma, the browser will suddenly THROW exceptions in random places that it never threw before just because at that spot you are doing something that default/loose JavaScript happily allows but strict JavaScript abhors!  You may have strictness violations hiding in seldom used calls in your code that will only throw an exception when they do eventually get run - say, in the production environment that your paying customers use!
If you are going to take the plunge, it is a good idea to apply ""use strict"" alongside comprehensive unit tests and a strictly configured JSHint build task that will give you some confidence that there is no dark corner of your module that will blow up horribly just because you've turned on Strict Mode.  Or, hey, here's another option:  just don't add ""use strict"" to any of your legacy code, it's probably safer that way, honestly.  DEFINITELY DO NOT add ""use strict"" to any modules you do not own or maintain, like third party modules.
I think even though it is a deadly caged animal, ""use strict"" can be good stuff, but you have to do it right.  The best time to go strict is when your project is greenfield and you are starting from scratch. Configure JSHint/JSLint with all the warnings and options cranked up as tight as your team can stomach, get a good build/test/assert system du jour rigged like Grunt+Karma+Chai, and only THEN start marking all your new modules as ""use strict"".  Be prepared to cure lots of niggly errors and warnings.  Make sure everyone understands the gravity by configuring the build to FAIL if JSHint/JSLint produces any violations.
My project was not a greenfield project when I adopted ""use strict"".  As a result, my IDE is full of red marks because I don't have ""use strict"" on half my modules, and JSHint complains about that.  It's a reminder to me about what refactoring I should do in the future.  My goal is to be red mark free due to all of my missing ""use strict"" statements, but that is years away now.
"
503093,506004,16260,"One does not simply redirect using jQuery
jQuery is not necessary, and window.location.replace(...) will best simulate an HTTP redirect.
window.location.replace(...) is better than using window.location.href, because replace() does not keep the originating page in the session history, meaning the user won't get stuck in a never-ending back-button fiasco.
If you want to simulate someone clicking on a link, use
location.href
If you want to simulate an HTTP redirect, use
location.replace
For example:
// similar behavior as an HTTP redirect
window.location.replace(""http://stackoverflow.com"");

// similar behavior as clicking on a link
window.location.href = ""http://stackoverflow.com"";

"
503093,1638449,1833,"WARNING: This answer has merely been provided as a possible solution; it is obviously not the best solution, as it requires jQuery. Instead, prefer the pure JavaScript solution.
$(location).prop('href', 'http://stackoverflow.com')

"
503093,11690095,829,"Standard ""vanilla"" JavaScript way to redirect a page
window.location.href = 'newPage.html';

Or more simply:  (since window is Global)
location.href = 'newPage.html';



If you are here because you are losing HTTP_REFERER when redirecting, keep reading:
(Otherwise ignore this last part)


The following section is for those using HTTP_REFERER as one of many security measures (although it isn't a great protective measure). If you're using Internet Explorer 8 or lower, these variables get lost when using any form of JavaScript page redirection (location.href,  etc.).
Below we are going to implement an alternative for IE8 & lower so that we don't lose HTTP_REFERER. Otherwise, you can almost always simply use window.location.href.
Testing against HTTP_REFERER (URL pasting, session, etc.) can help tell whether a request is legitimate.
(Note: there are also ways to work-around / spoof these referrers, as noted by droop's link in the comments)

Simple cross-browser testing solution (fallback to window.location.href for Internet Explorer 9+ and all other browsers)
Usage: redirect('anotherpage.aspx');
function redirect (url) {
    var ua        = navigator.userAgent.toLowerCase(),
        isIE      = ua.indexOf('msie') !== -1,
        version   = parseInt(ua.substr(4, 2), 10);

    // Internet Explorer 8 and lower
    if (isIE && version < 9) {
        var link = document.createElement('a');
        link.href = url;
        document.body.appendChild(link);
        link.click();
    }

    // All other browsers can use the standard window.location.href (they don't lose HTTP_REFERER like Internet Explorer 8 & lower does)
    else { 
        window.location.href = url; 
    }
}

"
503093,21396837,505,"There are lots of ways of doing this.
// window.location
window.location.replace('http://www.example.com')
window.location.assign('http://www.example.com')
window.location.href = 'http://www.example.com'
document.location.href = '/path'

// window.history
window.history.back()
window.history.go(-1)

// window.navigate; ONLY for old versions of Internet Explorer
window.navigate('top.jsp')


// Probably no bueno
self.location = 'http://www.example.com';
top.location = 'http://www.example.com';

// jQuery
$(location).attr('href','http://www.example.com')
$(window).attr('location','http://www.example.com')
$(location).prop('href', 'http://www.example.com')

"
503093,4001713,369,"This works for every browser:
window.location.href = 'your_url';

"
111102,111111,8329,"A closure is a pairing of:

A function and
A reference to that function's outer scope (lexical environment)

A lexical environment is part of every execution context (stack frame) and is a map between identifiers (i.e. local variable names) and values.
Every function in JavaScript maintains a reference to its outer lexical environment. This reference is used to configure the execution context created when a function is invoked. This reference enables code inside the function to ""see"" variables declared outside the function, regardless of when and where the function is called.
If a function was called by a function, which in turn was called by another function, then a chain of references to outer lexical environments is created. This chain is called the scope chain.
In the following code, inner forms a closure with the lexical environment of the execution context created when foo is invoked, closing over variable secret:


function foo() {
  const secret = Math.trunc(Math.random() * 100)
  return function inner() {
    console.log(`The secret number is ${secret}.`)
  }
}
const f = foo() // `secret` is not directly accessible from outside `foo`
f() // The only way to retrieve `secret` is to invoke `f`



In other words: in JavaScript, functions carry a reference to a private ""box of state"", to which only they (and any other functions declared within the same lexical environment) have access. This box of the state is invisible to the caller of the function, delivering an excellent mechanism for data-hiding and encapsulation.
And remember: functions in JavaScript can be passed around like variables (first-class functions), meaning these pairings of functionality and state can be passed around your program, similar to how you might pass an instance of a class around in C++.
If JavaScript did not have closures, then more states would have to be passed between functions explicitly, making parameter lists longer and code noisier.
So, if you want a function to always have access to a private piece of state, you can use a closure.
...and frequently we do want to associate the state with a function. For example, in Java or C++, when you add a private instance variable and a method to a class, you are associating the state with functionality.
In C and most other common languages, after a function returns, all the local variables are no longer accessible because the stack-frame is destroyed. In JavaScript, if you declare a function within another function, then the local variables of the outer function can remain accessible after returning from it. In this way, in the code above, secret remains available to the function object inner, after it has been returned from foo.
Uses of Closures
Closures are useful whenever you need a private state associated with a function. This is a very common scenario - and remember: JavaScript did not have a class syntax until 2015, and it still does not have a private field syntax. Closures meet this need.
Private Instance Variables
In the following code, the function toString closes over the details of the car.


function Car(manufacturer, model, year, color) {
  return {
    toString() {
      return `${manufacturer} ${model} (${year}, ${color})`
    }
  }
}

const car = new Car('Aston Martin', 'V8 Vantage', '2012', 'Quantum Silver')
console.log(car.toString())



Functional Programming
In the following code, the function inner closes over both fn and args.


function curry(fn) {
  const args = []
  return function inner(arg) {
    if(args.length === fn.length) return fn(...args)
    args.push(arg)
    return inner
  }
}

function add(a, b) {
  return a + b
}

const curriedAdd = curry(add)
console.log(curriedAdd(2)(3)()) // 5



Event-Oriented Programming
In the following code, function onClick closes over variable BACKGROUND_COLOR.


const $ = document.querySelector.bind(document)
const BACKGROUND_COLOR = 'rgba(200, 200, 242, 1)'

function onClick() {
  $('body').style.background = BACKGROUND_COLOR
}

$('button').addEventListener('click', onClick)
<button>Set background color</button>



Modularization
In the following example, all the implementation details are hidden inside an immediately executed function expression. The functions tick and toString close over the private state and functions they need to complete their work. Closures have enabled us to modularize and encapsulate our code.


let namespace = {};

(function foo(n) {
  let numbers = []

  function format(n) {
    return Math.trunc(n)
  }

  function tick() {
    numbers.push(Math.random() * 100)
  }

  function toString() {
    return numbers.map(format)
  }

  n.counter = {
    tick,
    toString
  }
}(namespace))

const counter = namespace.counter
counter.tick()
counter.tick()
console.log(counter.toString())



Examples
Example 1
This example shows that the local variables are not copied in the closure: the closure maintains a reference to the original variables themselves. It is as though the stack-frame stays alive in memory even after the outer function exits.


function foo() {
  let x = 42
  let inner = () => console.log(x)
  x = x + 1
  return inner
}

foo()() // logs 43



Example 2
In the following code, three methods log, increment, and update all close over the same lexical environment.
And every time createObject is called, a new execution context (stack frame) is created and a completely new variable x, and a new set of functions (log etc.) are created, that close over this new variable.


function createObject() {
  let x = 42;
  return {
    log() { console.log(x) },
    increment() { x++ },
    update(value) { x = value }
  }
}

const o = createObject()
o.increment()
o.log() // 43
o.update(5)
o.log() // 5
const p = createObject()
p.log() // 42



Example 3
If you are using variables declared using var, be careful you understand which variable you are closing over. Variables declared using var are hoisted. This is much less of a problem in modern JavaScript due to the introduction of let and const.
In the following code, each time around the loop, a new function inner is created, which closes over i. But because var i is hoisted outside the loop, all of these inner functions close over the same variable, meaning that the final value of i (3) is printed, three times.


function foo() {
  var result = []
  for (var i = 0; i < 3; i++) {
    result.push(function inner() { console.log(i) } )
  }

  return result
}

const result = foo()
// The following will print `3`, three times...
for (var i = 0; i < 3; i++) {
  result[i]() 
}



Final points:

Whenever a function is declared in JavaScript closure is created.
Returning a function from inside another function is the classic example of closure, because the state inside the outer function is implicitly available to the returned inner function, even after the outer function has completed execution.
Whenever you use eval() inside a function, a closure is used. The text you eval can reference local variables of the function, and in the non-strict mode, you can even create new local variables by using eval('var foo = …').
When you use new Function(…) (the Function constructor) inside a function, it does not close over its lexical environment: it closes over the global context instead. The new function cannot reference the local variables of the outer function.
A closure in JavaScript is like keeping a reference (NOT a copy) to the scope at the point of function declaration, which in turn keeps a reference to its outer scope, and so on, all the way to the global object at the top of the scope chain.
A closure is created when a function is declared; this closure is used to configure the execution context when the function is invoked.
A new set of local variables is created every time a function is called.

Links

Douglas Crockford's simulated private attributes and private methods for an object, using closures.
A great explanation of how closures can cause memory leaks in IE if you are not careful.
MDN documentation on JavaScript Closures.
The Beginner's Guide to JavaScript Closures.

"
111102,111200,4144,"Every function in JavaScript maintains a link to its outer lexical environment. A lexical environment is a map of all the names (eg. variables, parameters) within a scope, with their values.
So, whenever you see the function keyword, code inside that function has access to variables declared outside the function.


function foo(x) {
  var tmp = 3;

  function bar(y) {
    console.log(x + y + (++tmp)); // will log 16
  }

  bar(10);
}

foo(2);



This will log 16 because function bar closes over the parameter x and the variable tmp, both of which exist in the lexical environment of outer function foo.
Function bar, together with its link with the lexical environment of function foo is a closure. 
A function doesn't have to return in order to create a closure. Simply by virtue of its declaration, every function closes over its enclosing lexical environment, forming a closure.


function foo(x) {
  var tmp = 3;

  return function (y) {
    console.log(x + y + (++tmp)); // will also log 16
  }
}

var bar = foo(2);
bar(10); // 16
bar(10); // 17



The above function will also log 16, because the code inside bar can still refer to argument x and variable tmp, even though they are no longer directly in scope.
However, since tmp is still hanging around inside bar's closure, it is available to be incremented. It will be incremented each time you call bar.
The simplest example of a closure is this:


var a = 10;

function test() {
  console.log(a); // will output 10
  console.log(b); // will output 6
}
var b = 6;
test();



When a JavaScript function is invoked, a new execution context ec is created. Together with the function arguments and the target object, this execution context also receives a link to the lexical environment of the calling execution context, meaning the variables declared in the outer lexical environment (in the above example, both a and b) are available from ec.
Every function creates a closure because every function has a link to its outer lexical environment. 
Note that variables themselves are visible from within a closure, not copies.
"
111102,6472397,2610,"FOREWORD: this answer was written when the question was:

Like the old Albert said : ""If you can't explain it to a six-year old, you really don't understand it yourself.”. Well I tried to explain JS closures to a 27 years old friend and completely failed.
Can anybody consider that I am 6 and strangely interested in that subject ?

I'm pretty sure I was one of the only people that attempted to take the initial question literally. Since then, the question has mutated several times, so my answer may now seem incredibly silly & out of place. Hopefully the general idea of the story remains fun for some.

I'm a big fan of analogy and metaphor when explaining difficult concepts, so let me try my hand with a story.
Once upon a time:
There was a princess...
function princess() {

She lived in a wonderful world full of adventures. She met her Prince Charming, rode around her world on a unicorn, battled dragons, encountered talking animals, and many other fantastical things.
    var adventures = [];

    function princeCharming() { /* ... */ }

    var unicorn = { /* ... */ },
        dragons = [ /* ... */ ],
        squirrel = ""Hello!"";

    /* ... */

But she would always have to return back to her dull world of chores and grown-ups.
    return {

And she would often tell them of her latest amazing adventure as a princess.
        story: function() {
            return adventures[adventures.length - 1];
        }
    };
}

But all they would see is a little girl...
var littleGirl = princess();

...telling stories about magic and fantasy.
littleGirl.story();

And even though the grown-ups knew of real princesses, they would never believe in the unicorns or dragons because they could never see them. The grown-ups said that they only existed inside the little girl's imagination.
But we know the real truth; that the little girl with the princess inside...
...is really a princess with a little girl inside.
"
111102,7285658,813,"Taking the question seriously, we should find out what a typical 6-year-old is capable of cognitively, though admittedly, one who is interested in JavaScript is not so typical.  
On  Childhood Development: 5 to 7 Years  it says:

Your child will be able to follow two-step directions. For example, if you say to your child, ""Go to the kitchen and get me a trash bag"" they will be able to remember that direction.

We can use this example to explain closures, as follows:

The kitchen is a closure that has a local variable, called trashBags.  There is a function inside the kitchen called getTrashBag that gets one trash bag and returns it.

We can code this in JavaScript like this:


function makeKitchen() {
  var trashBags = ['A', 'B', 'C']; // only 3 at first

  return {
    getTrashBag: function() {
      return trashBags.pop();
    }
  };
}

var kitchen = makeKitchen();

console.log(kitchen.getTrashBag()); // returns trash bag C
console.log(kitchen.getTrashBag()); // returns trash bag B
console.log(kitchen.getTrashBag()); // returns trash bag A



Further points that explain why closures are interesting:

Each time makeKitchen() is called, a new closure is created with its own separate trashBags.
The trashBags variable is local to the inside of each kitchen and is not accessible outside, but the inner function on the getTrashBag property does have access to it.  
Every function call creates a closure, but there would be no need to keep the closure around unless an inner function, which has access to the inside of the closure, can be called from outside the closure.  Returning the object with the getTrashBag function does that here.

"
111102,15097817,626,"The Straw Man
I need to know how many times a button has been clicked and do something on every third click...
Fairly Obvious Solution


// Declare counter outside event handler's scope
var counter = 0;
var element = document.getElementById('button');

element.addEventListener(""click"", function() {
  // Increment outside counter
  counter++;

  if (counter === 3) {
    // Do something every third time
    console.log(""Third time's the charm!"");

    // Reset counter
    counter = 0;
  }
});
<button id=""button"">Click Me!</button>



Now this will work, but it does encroach into the outer scope by adding a variable, whose sole purpose is to keep track of the count. In some situations, this would be preferable as your outer application might need access to this information. But in this case, we are only changing every third click's behavior, so it is preferable to enclose this functionality inside the event handler.
Consider this option


var element = document.getElementById('button');

element.addEventListener(""click"", (function() {
  // init the count to 0
  var count = 0;

  return function(e) { // <- This function becomes the click handler
    count++; //    and will retain access to the above `count`

    if (count === 3) {
      // Do something every third time
      console.log(""Third time's the charm!"");

      //Reset counter
      count = 0;
    }
  };
})());
<button id=""button"">Click Me!</button>



Notice a few things here.
In the above example, I am using the closure behavior of JavaScript. This behavior allows any function to have access to the scope in which it was created, indefinitely. To practically apply this, I immediately invoke a function that returns another function, and because the function I'm returning has access to the internal count variable (because of the closure behavior explained above) this results in a private scope for usage by the resulting function... Not so simple? Let's dilute it down...
A simple one-line closure
//          _______________________Immediately invoked______________________
//         |                                                                |
//         |        Scope retained for use      ___Returned as the____      |
//         |       only by returned function   |    value of func     |     |
//         |             |            |        |                      |     |
//         v             v            v        v                      v     v
var func = (function() { var a = 'val'; return function() { alert(a); }; })();

All variables outside the returned function are available to the returned function, but they are not directly available to the returned function object...
func();  // Alerts ""val""
func.a;  // Undefined

Get it? So in our primary example, the count variable is contained within the closure and always available to the event handler, so it retains its state from click to click.
Also, this private variable state is fully accessible, for both readings and assigning to its private scoped variables.
There you go; you're now fully encapsulating this behavior.
Full Blog Post (including jQuery considerations)
"
11227809,11227902,34873,"You are a victim of branch prediction fail.

What is Branch Prediction?
Consider a railroad junction:

Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license.
Now for the sake of argument, suppose this is back in the 1800s - before long-distance or radio communication.
You are a blind operator of a junction and you hear a train coming. You have no idea which way it is supposed to go. You stop the train to ask the driver which direction they want. And then you set the switch appropriately.
Trains are heavy and have a lot of inertia, so they take forever to start up and slow down.
Is there a better way? You guess which direction the train will go!

If you guessed right, it continues on.
If you guessed wrong, the driver will stop, back up, and yell at you to flip the switch. Then it can restart down the other path.

If you guess right every time, the train will never have to stop.
If you guess wrong too often, the train will spend a lot of time stopping, backing up, and restarting.

Consider an if-statement: At the processor level, it is a branch instruction:

You are a processor and you see a branch. You have no idea which way it will go. What do you do? You halt execution and wait until the previous instructions are complete. Then you continue down the correct path.
Modern processors are complicated and have long pipelines. This means they take forever to ""warm up"" and ""slow down"".
Is there a better way? You guess which direction the branch will go!

If you guessed right, you continue executing.
If you guessed wrong, you need to flush the pipeline and roll back to the branch. Then you can restart down the other path.

If you guess right every time, the execution will never have to stop.
If you guess wrong too often, you spend a lot of time stalling, rolling back, and restarting.

This is branch prediction. I admit it's not the best analogy since the train could just signal the direction with a flag. But in computers, the processor doesn't know which direction a branch will go until the last moment.
How would you strategically guess to minimize the number of times that the train must back up and go down the other path? You look at the past history! If the train goes left 99% of the time, then you guess left. If it alternates, then you alternate your guesses. If it goes one way every three times, you guess the same...
In other words, you try to identify a pattern and follow it. This is more or less how branch predictors work.
Most applications have well-behaved branches. Therefore, modern branch predictors will typically achieve >90% hit rates. But when faced with unpredictable branches with no recognizable patterns, branch predictors are virtually useless.
Further reading: ""Branch predictor"" article on Wikipedia.

As hinted from above, the culprit is this if-statement:
if (data[c] >= 128)
    sum += data[c];

Notice that the data is evenly distributed between 0 and 255. When the data is sorted, roughly the first half of the iterations will not enter the if-statement. After that, they will all enter the if-statement.
This is very friendly to the branch predictor since the branch consecutively goes the same direction many times. Even a simple saturating counter will correctly predict the branch except for the few iterations after it switches direction.
Quick visualization:
T = branch taken
N = branch not taken

data[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ...
branch = N  N  N  N  N  ...   N    N    T    T    T  ...   T    T    T  ...

       = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT  (easy to predict)

However, when the data is completely random, the branch predictor is rendered useless, because it can't predict random data. Thus there will probably be around 50% misprediction (no better than random guessing).
data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118,  14, 150, 177, 182, ...
branch =   T,   T,   N,   T,   T,   T,   T,  N,   T,   N,   N,   T,   T,   T  ...

       = TTNTTTTNTNNTTT ...   (completely random - impossible to predict)


What can be done?
If the compiler isn't able to optimize the branch into a conditional move, you can try some hacks if you are willing to sacrifice readability for performance.
Replace:
if (data[c] >= 128)
    sum += data[c];

with:
int t = (data[c] - 128) >> 31;
sum += ~t & data[c];

This eliminates the branch and replaces it with some bitwise operations.
(Note that this hack is not strictly equivalent to the original if-statement. But in this case, it's valid for all the input values of data[].)
Benchmarks: Core i7 920 @ 3.5 GHz
C++ - Visual Studio 2010 - x64 Release




Scenario
Time (seconds)




Branching - Random data
11.777


Branching - Sorted data
2.352


Branchless - Random data
2.564


Branchless - Sorted data
2.587




Java - NetBeans 7.1.1 JDK 7 - x64




Scenario
Time (seconds)




Branching - Random data
10.93293813


Branching - Sorted data
5.643797077


Branchless - Random data
3.113581453


Branchless - Sorted data
3.186068823




Observations:

With the Branch: There is a huge difference between the sorted and unsorted data.
With the Hack: There is no difference between sorted and unsorted data.
In the C++ case, the hack is actually a tad slower than with the branch when the data is sorted.

A general rule of thumb is to avoid data-dependent branching in critical loops (such as in this example).

Update:

GCC 4.6.1 with -O3 or -ftree-vectorize on x64 is able to generate a conditional move, so there is no difference between the sorted and unsorted data - both are fast.
(Or somewhat fast: for the already-sorted case, cmov can be slower especially if GCC puts it on the critical path instead of just add, especially on Intel before Broadwell where cmov has 2 cycle latency: gcc optimization flag -O3 makes code slower than -O2)

VC++ 2010 is unable to generate conditional moves for this branch even under /Ox.

Intel C++ Compiler (ICC) 11 does something miraculous. It interchanges the two loops, thereby hoisting the unpredictable branch to the outer loop. Not only is it immune to the mispredictions, it's also twice as fast as whatever VC++ and GCC can generate! In other words, ICC took advantage of the test-loop to defeat the benchmark...

If you give the Intel compiler the branchless code, it just outright vectorizes it... and is just as fast as with the branch (with the loop interchange).


This goes to show that even mature modern compilers can vary wildly in their ability to optimize code...
"
11227809,11227877,4666,"Branch prediction.
With a sorted array, the condition data[c] >= 128 is first false for a streak of values, then becomes true for all later values. That's easy to predict. With an unsorted array, you pay for the branching cost.
"
11227809,11237235,3804,"The reason why performance improves drastically when the data is sorted is that the branch prediction penalty is removed, as explained beautifully in Mysticial's answer.
Now, if we look at the code
if (data[c] >= 128)
    sum += data[c];

we can find that the meaning of this particular if... else... branch is to add something when a condition is satisfied. This type of branch can be easily transformed into a conditional move statement, which would be compiled into a conditional move instruction: cmovl, in an x86 system. The branch and thus the potential branch prediction penalty is removed.
In C, thus C++, the statement, which would compile directly (without any optimization) into the conditional move instruction in x86, is the ternary operator ... ? ... : .... So we rewrite the above statement into an equivalent one:
sum += data[c] >=128 ? data[c] : 0;

While maintaining readability, we can check the speedup factor.
On an Intel Core i7-2600K @ 3.4 GHz and Visual Studio 2010 Release Mode, the benchmark is:
x86




Scenario
Time (seconds)




Branching - Random data
8.885


Branching - Sorted data
1.528


Branchless - Random data
3.716


Branchless - Sorted data
3.71




x64




Scenario
Time (seconds)




Branching - Random data
11.302


Branching - Sorted data
1.830


Branchless - Random data
2.736


Branchless - Sorted data
2.737




The result is robust in multiple tests. We get a great speedup when the branch result is unpredictable, but we suffer a little bit when it is predictable. In fact, when using a conditional move, the performance is the same regardless of the data pattern.
Now let's look more closely by investigating the x86 assembly they generate. For simplicity, we use two functions max1 and max2.
max1 uses the conditional branch if... else ...:
int max1(int a, int b) {
    if (a > b)
        return a;
    else
        return b;
}

max2 uses the ternary operator ... ? ... : ...:
int max2(int a, int b) {
    return a > b ? a : b;
}

On an x86-64 machine, GCC -S generates the assembly below.
:max1
    movl    %edi, -4(%rbp)
    movl    %esi, -8(%rbp)
    movl    -4(%rbp), %eax
    cmpl    -8(%rbp), %eax
    jle     .L2
    movl    -4(%rbp), %eax
    movl    %eax, -12(%rbp)
    jmp     .L4
.L2:
    movl    -8(%rbp), %eax
    movl    %eax, -12(%rbp)
.L4:
    movl    -12(%rbp), %eax
    leave
    ret

:max2
    movl    %edi, -4(%rbp)
    movl    %esi, -8(%rbp)
    movl    -4(%rbp), %eax
    cmpl    %eax, -8(%rbp)
    cmovge  -8(%rbp), %eax
    leave
    ret

max2 uses much less code due to the usage of instruction cmovge. But the real gain is that max2 does not involve branch jumps, jmp, which would have a significant performance penalty if the predicted result is not right.
So why does a conditional move perform better?
In a typical x86 processor, the execution of an instruction is divided into several stages. Roughly, we have different hardware to deal with different stages. So we do not have to wait for one instruction to finish to start a new one. This is called pipelining.
In a branch case, the following instruction is determined by the preceding one, so we cannot do pipelining. We have to either wait or predict.
In a conditional move case, the execution of conditional move instruction is divided into several stages, but the earlier stages like Fetch and Decode do not depend on the result of the previous instruction; only the latter stages need the result. Thus, we wait a fraction of one instruction's execution time. This is why the conditional move version is slower than the branch when the prediction is easy.
The book Computer Systems: A Programmer's Perspective, second edition explains this in detail. You can check Section 3.6.6 for Conditional Move Instructions, entire Chapter 4 for Processor Architecture, and Section 5.11.2 for special treatment for Branch Prediction and Misprediction Penalties.
Sometimes, some modern compilers can optimize our code to assembly with better performance, and sometimes some compilers can't (the code in question is using Visual Studio's native compiler). Knowing the performance difference between a branch and a conditional move when unpredictable can help us write code with better performance when the scenario gets so complex that the compiler can not optimize them automatically.
"
11227809,11303693,2616,"If you are curious about even more optimizations that can be done to this code, consider this:
Starting with the original loop:
for (unsigned i = 0; i < 100000; ++i)
{
    for (unsigned j = 0; j < arraySize; ++j)
    {
        if (data[j] >= 128)
            sum += data[j];
    }
}

With loop interchange, we can safely change this loop to:
for (unsigned j = 0; j < arraySize; ++j)
{
    for (unsigned i = 0; i < 100000; ++i)
    {
        if (data[j] >= 128)
            sum += data[j];
    }
}

Then, you can see that the if conditional is constant throughout the execution of the i loop, so you can hoist the if out:
for (unsigned j = 0; j < arraySize; ++j)
{
    if (data[j] >= 128)
    {
        for (unsigned i = 0; i < 100000; ++i)
        {
            sum += data[j];
        }
    }
}

Then, you see that the inner loop can be collapsed into one single expression, assuming the floating point model allows it (/fp:fast is thrown, for example)
for (unsigned j = 0; j < arraySize; ++j)
{
    if (data[j] >= 128)
    {
        sum += data[j] * 100000;
    }
}

That one is 100,000 times faster than before.
"
11227809,12853037,2166,"No doubt some of us would be interested in ways of identifying code that is problematic for the CPU's branch-predictor. The Valgrind tool cachegrind has a branch-predictor simulator, enabled by using the --branch-sim=yes flag. Running it over the examples in this question, with the number of outer loops reduced to 10000 and compiled with g++, gives these results:
Sorted:
==32551== Branches:        656,645,130  (  656,609,208 cond +    35,922 ind)
==32551== Mispredicts:         169,556  (      169,095 cond +       461 ind)
==32551== Mispred rate:            0.0% (          0.0%     +       1.2%   )

Unsorted:
==32555== Branches:        655,996,082  (  655,960,160 cond +  35,922 ind)
==32555== Mispredicts:     164,073,152  (  164,072,692 cond +     460 ind)
==32555== Mispred rate:           25.0% (         25.0%     +     1.2%   )

Drilling down into the line-by-line output produced by cg_annotate we see for the loop in question:
Sorted:
          Bc    Bcm Bi Bim
      10,001      4  0   0      for (unsigned i = 0; i < 10000; ++i)
           .      .  .   .      {
           .      .  .   .          // primary loop
 327,690,000 10,016  0   0          for (unsigned c = 0; c < arraySize; ++c)
           .      .  .   .          {
 327,680,000 10,006  0   0              if (data[c] >= 128)
           0      0  0   0                  sum += data[c];
           .      .  .   .          }
           .      .  .   .      }

Unsorted:
          Bc         Bcm Bi Bim
      10,001           4  0   0      for (unsigned i = 0; i < 10000; ++i)
           .           .  .   .      {
           .           .  .   .          // primary loop
 327,690,000      10,038  0   0          for (unsigned c = 0; c < arraySize; ++c)
           .           .  .   .          {
 327,680,000 164,050,007  0   0              if (data[c] >= 128)
           0           0  0   0                  sum += data[c];
           .           .  .   .          }
           .           .  .   .      }

This lets you easily identify the problematic line - in the unsorted version the if (data[c] >= 128) line is causing 164,050,007 mispredicted conditional branches (Bcm) under cachegrind's branch-predictor model, whereas it's only causing 10,006 in the sorted version.

Alternatively, on Linux you can use the performance counters subsystem to accomplish the same task, but with native performance using CPU counters.
perf stat ./sumtest_sorted

Sorted:
 Performance counter stats for './sumtest_sorted':

  11808.095776 task-clock                #    0.998 CPUs utilized          
         1,062 context-switches          #    0.090 K/sec                  
            14 CPU-migrations            #    0.001 K/sec                  
           337 page-faults               #    0.029 K/sec                  
26,487,882,764 cycles                    #    2.243 GHz                    
41,025,654,322 instructions              #    1.55  insns per cycle        
 6,558,871,379 branches                  #  555.455 M/sec                  
       567,204 branch-misses             #    0.01% of all branches        

  11.827228330 seconds time elapsed

Unsorted:
 Performance counter stats for './sumtest_unsorted':

  28877.954344 task-clock                #    0.998 CPUs utilized          
         2,584 context-switches          #    0.089 K/sec                  
            18 CPU-migrations            #    0.001 K/sec                  
           335 page-faults               #    0.012 K/sec                  
65,076,127,595 cycles                    #    2.253 GHz                    
41,032,528,741 instructions              #    0.63  insns per cycle        
 6,560,579,013 branches                  #  227.183 M/sec                  
 1,646,394,749 branch-misses             #   25.10% of all branches        

  28.935500947 seconds time elapsed

It can also do source code annotation with dissassembly.
perf record -e branch-misses ./sumtest_unsorted
perf annotate -d sumtest_unsorted

 Percent |      Source code & Disassembly of sumtest_unsorted
------------------------------------------------
...
         :                      sum += data[c];
    0.00 :        400a1a:       mov    -0x14(%rbp),%eax
   39.97 :        400a1d:       mov    %eax,%eax
    5.31 :        400a1f:       mov    -0x20040(%rbp,%rax,4),%eax
    4.60 :        400a26:       cltq   
    0.00 :        400a28:       add    %rax,-0x30(%rbp)
...

See the performance tutorial for more details.
"
40480,40523,7021,"The terms ""pass-by-value"" and ""pass-by-reference"" have special, precisely defined meanings in computer science. These meanings differ from the intuition many people have when first hearing the terms. Much of the confusion in this discussion seems to come from this fact.
The terms ""pass-by-value"" and ""pass-by-reference"" are talking about variables. Pass-by-value means that the value of a variable is passed to a function/method. Pass-by-reference means that a reference to that variable is passed to the function. The latter gives the function a way to change the contents of the variable.
By those definitions, Java is always pass-by-value.  Unfortunately, when we deal with variables holding objects we are really dealing with object-handles called references which are passed-by-value as well.  This terminology and semantics easily confuse many beginners.
It goes like this:
public static void main(String[] args) {
    Dog aDog = new Dog(""Max"");
    Dog oldDog = aDog;

    // we pass the object to foo
    foo(aDog);
    // aDog variable is still pointing to the ""Max"" dog when foo(...) returns
    aDog.getName().equals(""Max""); // true
    aDog.getName().equals(""Fifi""); // false
    aDog == oldDog; // true
}

public static void foo(Dog d) {
    d.getName().equals(""Max""); // true
    // change d inside of foo() to point to a new Dog instance ""Fifi""
    d = new Dog(""Fifi"");
    d.getName().equals(""Fifi""); // true
}

In the example above aDog.getName() will still return ""Max"". The value aDog within main is not changed in the function foo with the Dog ""Fifi"" as the object reference is passed by value. If it were passed by reference, then the aDog.getName() in main would return ""Fifi"" after the call to foo.
Likewise:
public static void main(String[] args) {
    Dog aDog = new Dog(""Max"");
    Dog oldDog = aDog;

    foo(aDog);
    // when foo(...) returns, the name of the dog has been changed to ""Fifi""
    aDog.getName().equals(""Fifi""); // true
    // but it is still the same dog:
    aDog == oldDog; // true
}

public static void foo(Dog d) {
    d.getName().equals(""Max""); // true
    // this changes the name of d to be ""Fifi""
    d.setName(""Fifi"");
}

In the above example, Fifi is the dog's name after call to foo(aDog) because the object's name was set inside of foo(...). Any operations that foo performs on d are such that, for all practical purposes, they are performed on aDog, but it is not possible to change the value of the variable aDog itself.
For more information on pass by reference and pass by value, consult the following answer: https://stackoverflow.com/a/430958/6005228. This explains more thoroughly the semantics and history behind the two and also explains why Java and many other modern languages appear to do both in certain cases.
"
40480,73021,3580,"I just noticed you referenced my article.
The Java Spec says that everything in Java is pass-by-value. There is no such thing as ""pass-by-reference"" in Java.
The key to understanding this is that something like
Dog myDog;

is not a Dog; it's actually a pointer to a Dog. The use of the term ""reference"" in Java is very misleading and is what causes most of the confusion here. What they call ""references"" act/feel more like what we'd call ""pointers"" in most other languages.
What that means, is when you have
Dog myDog = new Dog(""Rover"");
foo(myDog);

you're essentially passing the address of the created Dog object to the foo method.
(I say essentially because Java pointers/references aren't direct addresses, but it's easiest to think of them that way.)
Suppose the Dog object resides at memory address 42. This means we pass 42 to the method.
if the Method were defined as
public void foo(Dog someDog) {
    someDog.setName(""Max"");     // AAA
    someDog = new Dog(""Fifi"");  // BBB
    someDog.setName(""Rowlf"");   // CCC
}

let's look at what's happening.

the parameter someDog is set to the value 42
at line ""AAA""

someDog is followed to the Dog it points to (the Dog object at address 42)
that Dog (the one at address 42) is asked to change his name to Max


at line ""BBB""

a new Dog is created. Let's say he's at address 74
we assign the parameter someDog to 74


at line ""CCC""

someDog is followed to the Dog it points to (the Dog object at address 74)
that Dog (the one at address 74) is asked to change his name to Rowlf


then, we return

Now let's think about what happens outside the method:
Did myDog change?
There's the key.
Keeping in mind that myDog is a pointer, and not an actual Dog, the answer is NO. myDog still has the value 42; it's still pointing to the original Dog (but note that because of line ""AAA"", its name is now ""Max"" - still the same Dog; myDog's value has not changed.)
It's perfectly valid to follow an address and change what's at the end of it; that does not change the variable, however.
Java works exactly like C. You can assign a pointer, pass the pointer to a method, follow the pointer in the method and change the data that was pointed to. However, the caller will not see any changes you make to where that pointer points. (In a language with pass-by-reference semantics, the method function can change the pointer and the caller will see that change.)
In C++, Ada, Pascal and other languages that support pass-by-reference, you can actually change the variable that was passed.
If Java had pass-by-reference semantics, the foo method we defined above would have changed where myDog was pointing when it assigned someDog on line BBB.
Think of reference parameters as being aliases for the variable passed in. When that alias is assigned, so is the variable that was passed in.
Update
A discussion in the comments warrants some clarification...
In C, you can write
void swap(int *x, int *y) {
    int t = *x;
    *x = *y;
    *y = t;
}

int x = 1;
int y = 2;
swap(&x, &y);

This is not a special case in C. Both languages use pass-by-value semantics. Here the call site is creating additional data structure to assist the function to access and manipulate data.
The function is being passed pointers to data, and follows those pointers to access and modify that data.
A similar approach in Java, where the caller sets up assisting structure, might be:
void swap(int[] x, int[] y) {
    int temp = x[0];
    x[0] = y[0];
    y[0] = temp;
}

int[] x = {1};
int[] y = {2};
swap(x, y);

(or if you wanted both examples to demonstrate features the other language doesn't have, create a mutable IntWrapper class to use in place of the arrays)
In these cases, both C and Java are simulating pass-by-reference. They're still both passing values (pointers to ints or arrays), and following those pointers inside the called function to manipulate the data.
Pass-by-reference is all about the function declaration/definition, and how it handles its parameters. Reference semantics apply to every call to that function, and the call site only needs to pass variables, no additional data structure.
These simulations require the call site and the function to cooperate. No doubt it's useful, but it's still pass-by-value.
"
40480,12429953,2118,"Java always passes arguments by value, NOT by reference.

Let me explain this through an example:
public class Main {

     public static void main(String[] args) {
          Foo f = new Foo(""f"");
          changeReference(f); // It won't change the reference!
          modifyReference(f); // It will modify the object that the reference variable ""f"" refers to!
     }

     public static void changeReference(Foo a) {
          Foo b = new Foo(""b"");
          a = b;
     }

     public static void modifyReference(Foo c) {
          c.setAttribute(""c"");
     }

}

I will explain this in steps:

Declaring a reference named f of type Foo and assign it a new object of type Foo with an attribute ""f"".
Foo f = new Foo(""f"");



From the method side, a reference of type Foo with a name a is declared and it's initially assigned null.
public static void changeReference(Foo a)



As you call the method changeReference, the reference a will be assigned the object which is passed as an argument.
changeReference(f);



Declaring a reference named b of type Foo and assign it a new object of type Foo with an attribute ""b"".
Foo b = new Foo(""b"");



a = b makes a new assignment to the reference a, not f, of the object whose attribute is ""b"".


As you call modifyReference(Foo c) method, a reference c is created and assigned the object with attribute ""f"".


c.setAttribute(""c""); will change the attribute of the object that reference c points to it, and it's the same object that reference f points to it.



"
40480,40499,857,"Java is always pass by value, with no exceptions, ever.
So how is it that anyone can be at all confused by this, and believe that Java is pass by reference, or think they have an example of Java acting as pass by reference? The key point is that Java never provides direct access to the values of objects themselves, in any circumstances. The only access to objects is through a reference to that object. Because Java objects are always accessed through a reference, rather than directly, it is common to talk about fields and variables and method arguments as being objects, when pedantically they are only references to objects. The confusion stems from this (strictly speaking, incorrect) change in nomenclature.
So, when calling a method

For primitive arguments (int, long, etc.), the pass by value is the actual value of the primitive (for example, 3).
For objects, the pass by value is the value of the reference to the object.

So if you have doSomething(foo) and public void doSomething(Foo foo) { .. } the two Foos have copied references that point to the same objects.
Naturally, passing by value a reference to an object looks very much like (and is indistinguishable in practice from) passing an object by reference.
"
40480,7034719,799,"This will give you some insights of how Java really works to the point that in your next discussion about Java passing by reference or passing by value you'll just smile :-)
Step one please erase from your mind that word that starts with 'p' ""_ _ _ _ _ _ _"", especially if you come from other programming languages. Java and 'p' cannot be written in the same book, forum, or even txt.
Step two remember that when you pass an Object into a method you're passing the Object reference and not the Object itself.

Student: Master, does this mean that Java is pass-by-reference?
Master: Grasshopper, No.

Now think of what an Object's reference/variable does/is:

A variable holds the bits that tell the JVM how to get to the referenced Object in memory (Heap).
When passing arguments to a method you ARE NOT passing the reference variable, but a copy of the bits in the reference variable. Something like this: 3bad086a. 3bad086a represents a way to get to the passed object.
So you're just passing 3bad086a that it's the value of the reference.
You're passing the value of the reference and not the reference itself (and not the object).
This value is actually COPIED and given to the method. 

In the following (please don't try to compile/execute this...):
1. Person person;
2. person = new Person(""Tom"");
3. changeName(person);
4.
5. //I didn't use Person person below as an argument to be nice
6. static void changeName(Person anotherReferenceToTheSamePersonObject) {
7.     anotherReferenceToTheSamePersonObject.setName(""Jerry"");
8. }

What happens?

The variable person is created in line #1 and it's null at the beginning.
A new Person Object is created in line #2, stored in memory, and the variable person is given the reference to the Person object. That is, its address. Let's say 3bad086a.
The variable person holding the address of the Object is passed to the function in line #3.
In line #4 you can listen to the sound of silence 
Check the comment on line #5
A method local variable -anotherReferenceToTheSamePersonObject- is created and then comes the magic in line #6:


The variable/reference person is copied bit-by-bit and passed to anotherReferenceToTheSamePersonObject inside the function.
No new instances of Person are created.
Both ""person"" and ""anotherReferenceToTheSamePersonObject"" hold the same value of 3bad086a.
Don't try this but person==anotherReferenceToTheSamePersonObject would be true.
Both variables have IDENTICAL COPIES of the reference and they both refer to the same Person Object, the SAME Object on the Heap and NOT A COPY.


A picture is worth a thousand words:

Note that the anotherReferenceToTheSamePersonObject arrows is directed towards the Object and not towards the variable person!
If you didn't get it then just trust me and remember that it's better to say that Java is pass by value. Well, pass by reference value. Oh well, even better is pass-by-copy-of-the-variable-value! ;)
Now feel free to hate me but note that given this there is no difference between passing primitive data types and Objects when talking about method arguments.
You always pass a copy of the bits of the value of the reference!

If it's a primitive data type these bits will contain the value of the primitive data type itself.
If it's an Object the bits will contain the value of the address that tells the JVM how to get to the Object.


Java is pass-by-value because inside a method you can modify the referenced Object as much as you want but no matter how hard you try you'll never be able to modify the passed variable that will keep referencing (not p _ _ _ _ _ _ _) the same Object no matter what!



The changeName function above will never be able to modify the actual content (the bit values) of the passed reference. In other word changeName cannot make Person person refer to another Object.


Of course you can cut it short and just say that  Java is pass-by-value!
"
6841333,6841479,11854,"It's a time zone change on December 31st in Shanghai.
See this page for details of 1927 in Shanghai. Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds. So ""1927-12-31 23:54:08"" actually happened twice, and it looks like Java is parsing it as the later possible instant for that local date/time - hence the difference.
Just another episode in the often weird and wonderful world of time zones.
If rebuilt with version 2013a of TZDB, The original question would no longer demonstrate quite the same behaviour. In 2013a, the result would be 358 seconds, with a transition time of 23:54:03 instead of 23:54:08.
I only noticed this because I'm collecting questions like this in Noda Time, in the form of unit tests... The test has now been changed, but it just goes to show - not even historical data is safe.
In TZDB 2014f, the time of the change has moved to 1900-12-31, and it's now a mere 343 second change (so the time between t and t+1 is 344 seconds, if you see what I mean).

To answer a question around a transition at 1900... it looks like the Java time zone implementation treats all time zones as simply being in their standard time for any instant before the start of 1900 UTC:
import java.util.TimeZone;

public class Test {
    public static void main(String[] args) throws Exception {
        long startOf1900Utc = -2208988800000L;
        for (String id : TimeZone.getAvailableIDs()) {
            TimeZone zone = TimeZone.getTimeZone(id);
            if (zone.getRawOffset() != zone.getOffset(startOf1900Utc - 1)) {
                System.out.println(id);
            }
        }
    }
}

The code above produces no output on my Windows machine. So any time zone which has any offset other than its standard one at the start of 1900 will count that as a transition. TZDB itself has some data going back earlier than that, and doesn't rely on any idea of a ""fixed"" standard time (which is what getRawOffset assumes to be a valid concept) so other libraries needn't introduce this artificial transition.
"
6841333,6841572,1739,"You've encountered a local time discontinuity:

When local standard time was about to reach Sunday, 1. January 1928,
  00:00:00 clocks were turned backward 0:05:52 hours to Saturday, 31.
  December 1927, 23:54:08 local standard time instead

This is not particularly strange and has happened pretty much everywhere at one time or another as timezones were switched or changed due to political or administrative actions.
"
6841333,6858453,744,"The moral of this strangeness is:

Use dates and times in UTC wherever possible.
If you can not display a date or time in UTC, always indicate the time-zone.
If you can not require an input date/time in UTC, require an explicitly indicated time-zone.

"
6841333,6884599,416,"When incrementing time you should convert back to UTC and then add or subtract. Use the local time only for display.
This way you will be able to walk through any periods where hours or minutes happen twice.
If you converted to UTC, add each second, and convert to local time for display. You would go through 11:54:08 p.m. LMT - 11:59:59 p.m. LMT and then 11:54:08 p.m. CST - 11:59:59 p.m. CST.
"
6841333,10612572,348,"Instead of converting each date, you can use the following code: 
long difference = (sDt4.getTime() - sDt3.getTime()) / 1000;
System.out.println(difference);

And then see that the result is:
1

"
309424,35446009,3491,"To summarize the other answers, I found 11 main ways to do this (see below). And I wrote some performance tests (see results below):
Ways to convert an InputStream to a String:

Using IOUtils.toString (Apache Utils)
 String result = IOUtils.toString(inputStream, StandardCharsets.UTF_8);


Using CharStreams (Guava)
 String result = CharStreams.toString(new InputStreamReader(
       inputStream, Charsets.UTF_8));


Using Scanner (JDK)
 Scanner s = new Scanner(inputStream).useDelimiter(""\\A"");
 String result = s.hasNext() ? s.next() : """";


Using Stream API (Java 8). Warning: This solution converts different line breaks (like \r\n) to \n.
 String result = new BufferedReader(new InputStreamReader(inputStream))
   .lines().collect(Collectors.joining(""\n""));


Using parallel Stream API (Java 8). Warning: This solution converts different line breaks (like \r\n) to \n.
 String result = new BufferedReader(new InputStreamReader(inputStream))
    .lines().parallel().collect(Collectors.joining(""\n""));


Using InputStreamReader and StringBuilder (JDK)
 int bufferSize = 1024;
 char[] buffer = new char[bufferSize];
 StringBuilder out = new StringBuilder();
 Reader in = new InputStreamReader(stream, StandardCharsets.UTF_8);
 for (int numRead; (numRead = in.read(buffer, 0, buffer.length)) > 0; ) {
     out.append(buffer, 0, numRead);
 }
 return out.toString();


Using StringWriter and IOUtils.copy (Apache Commons)
 StringWriter writer = new StringWriter();
 IOUtils.copy(inputStream, writer, ""UTF-8"");
 return writer.toString();


Using ByteArrayOutputStream and inputStream.read (JDK)
 ByteArrayOutputStream result = new ByteArrayOutputStream();
 byte[] buffer = new byte[1024];
 for (int length; (length = inputStream.read(buffer)) != -1; ) {
     result.write(buffer, 0, length);
 }
 // StandardCharsets.UTF_8.name() > JDK 7
 return result.toString(""UTF-8"");


Using BufferedReader (JDK). Warning: This solution converts different line breaks (like \n\r) to line.separator system property (for example, in Windows to ""\r\n"").
 String newLine = System.getProperty(""line.separator"");
 BufferedReader reader = new BufferedReader(
         new InputStreamReader(inputStream));
 StringBuilder result = new StringBuilder();
 for (String line; (line = reader.readLine()) != null; ) {
     if (result.length() > 0) {
         result.append(newLine);
     }
     result.append(line);
 }
 return result.toString();


Using BufferedInputStream and ByteArrayOutputStream (JDK)
BufferedInputStream bis = new BufferedInputStream(inputStream);
ByteArrayOutputStream buf = new ByteArrayOutputStream();
for (int result = bis.read(); result != -1; result = bis.read()) {
    buf.write((byte) result);
}
// StandardCharsets.UTF_8.name() > JDK 7
return buf.toString(""UTF-8"");


Using inputStream.read() and StringBuilder (JDK). Warning: This solution has problems with Unicode, for example with Russian text (works correctly only with non-Unicode text)
StringBuilder sb = new StringBuilder();
for (int ch; (ch = inputStream.read()) != -1; ) {
    sb.append((char) ch);
}
return sb.toString();



Warning:

Solutions 4, 5 and 9 convert different line breaks to one.

Solution 11 can't work correctly with Unicode text


Performance tests
Performance tests for small String (length = 175), url in github (mode = Average Time, system = Linux, score 1,343 is the best):
              Benchmark                         Mode  Cnt   Score   Error  Units
 8. ByteArrayOutputStream and read (JDK)        avgt   10   1,343 ± 0,028  us/op
 6. InputStreamReader and StringBuilder (JDK)   avgt   10   6,980 ± 0,404  us/op
10. BufferedInputStream, ByteArrayOutputStream  avgt   10   7,437 ± 0,735  us/op
11. InputStream.read() and StringBuilder (JDK)  avgt   10   8,977 ± 0,328  us/op
 7. StringWriter and IOUtils.copy (Apache)      avgt   10  10,613 ± 0,599  us/op
 1. IOUtils.toString (Apache Utils)             avgt   10  10,605 ± 0,527  us/op
 3. Scanner (JDK)                               avgt   10  12,083 ± 0,293  us/op
 2. CharStreams (guava)                         avgt   10  12,999 ± 0,514  us/op
 4. Stream Api (Java 8)                         avgt   10  15,811 ± 0,605  us/op
 9. BufferedReader (JDK)                        avgt   10  16,038 ± 0,711  us/op
 5. parallel Stream Api (Java 8)                avgt   10  21,544 ± 0,583  us/op

Performance tests for big String (length = 50100), url in github (mode = Average Time, system = Linux, score 200,715 is the best):
               Benchmark                        Mode  Cnt   Score        Error  Units
 8. ByteArrayOutputStream and read (JDK)        avgt   10   200,715 ±   18,103  us/op
 1. IOUtils.toString (Apache Utils)             avgt   10   300,019 ±    8,751  us/op
 6. InputStreamReader and StringBuilder (JDK)   avgt   10   347,616 ±  130,348  us/op
 7. StringWriter and IOUtils.copy (Apache)      avgt   10   352,791 ±  105,337  us/op
 2. CharStreams (guava)                         avgt   10   420,137 ±   59,877  us/op
 9. BufferedReader (JDK)                        avgt   10   632,028 ±   17,002  us/op
 5. parallel Stream Api (Java 8)                avgt   10   662,999 ±   46,199  us/op
 4. Stream Api (Java 8)                         avgt   10   701,269 ±   82,296  us/op
10. BufferedInputStream, ByteArrayOutputStream  avgt   10   740,837 ±    5,613  us/op
 3. Scanner (JDK)                               avgt   10   751,417 ±   62,026  us/op
11. InputStream.read() and StringBuilder (JDK)  avgt   10  2919,350 ± 1101,942  us/op

Graphs (performance tests depending on Input Stream length in Windows 7 system)

Performance test (Average Time) depending on Input Stream length in Windows 7 system:
 length  182    546     1092    3276    9828    29484   58968

 test8  0.38    0.938   1.868   4.448   13.412  36.459  72.708
 test4  2.362   3.609   5.573   12.769  40.74   81.415  159.864
 test5  3.881   5.075   6.904   14.123  50.258  129.937 166.162
 test9  2.237   3.493   5.422   11.977  45.98   89.336  177.39
 test6  1.261   2.12    4.38    10.698  31.821  86.106  186.636
 test7  1.601   2.391   3.646   8.367   38.196  110.221 211.016
 test1  1.529   2.381   3.527   8.411   40.551  105.16  212.573
 test3  3.035   3.934   8.606   20.858  61.571  118.744 235.428
 test2  3.136   6.238   10.508  33.48   43.532  118.044 239.481
 test10 1.593   4.736   7.527   20.557  59.856  162.907 323.147
 test11 3.913   11.506  23.26   68.644  207.591 600.444 1211.545

"
309424,309448,2756,"A nice way to do this is using Apache Commons IOUtils to copy the InputStream into a StringWriter... Something like
StringWriter writer = new StringWriter();
IOUtils.copy(inputStream, writer, encoding);
String theString = writer.toString();

or even
// NB: does not close inputStream, you'll have to use try-with-resources for that
String theString = IOUtils.toString(inputStream, encoding);

Alternatively, you could use ByteArrayOutputStream if you don't want to mix your Streams and Writers.
"
309424,5445161,2363,"Here's a way using only the standard Java library (note that the stream is not closed, your mileage may vary).
static String convertStreamToString(java.io.InputStream is) {
    java.util.Scanner s = new java.util.Scanner(is).useDelimiter(""\\A"");
    return s.hasNext() ? s.next() : """";
}

I learned this trick from ""Stupid Scanner tricks"" article. The reason it works is because Scanner iterates over tokens in the stream, and in this case we separate tokens using ""beginning of the input boundary"" (\A), thus giving us only one token for the entire contents of the stream.
Note, if you need to be specific about the input stream's encoding, you can provide the second argument to Scanner constructor that indicates what character set to use (e.g. ""UTF-8"").
Hat tip goes also to Jacob, who once pointed me to the said article.
"
309424,350723,890,"Apache Commons allows:
String myString = IOUtils.toString(myInputStream, ""UTF-8"");

Of course, you could choose other character encodings besides UTF-8.
Also see: (documentation)
"
309424,309718,321,"Taking into account file one should first get a java.io.Reader instance. This can then be read and added to a StringBuilder (we don't need StringBuffer if we are not accessing it in multiple threads, and StringBuilder is faster). The trick here is that we work in blocks, and as such don't need other buffering streams. The block size is parameterized for run-time performance optimization.
public static String slurp(final InputStream is, final int bufferSize) {
    final char[] buffer = new char[bufferSize];
    final StringBuilder out = new StringBuilder();
    try (Reader in = new InputStreamReader(is, ""UTF-8"")) {
        for (;;) {
            int rsz = in.read(buffer, 0, buffer.length);
            if (rsz < 0)
                break;
            out.append(buffer, 0, rsz);
        }
    }
    catch (UnsupportedEncodingException ex) {
        /* ... */
    }
    catch (IOException ex) {
        /* ... */
    }
    return out.toString();
}

"
271526,271874,2854,"This to me sounds like a reasonably common problem that junior to intermediate developers tend to face at some point: they either don't know or don't trust the contracts they are participating in and defensively overcheck for nulls.  Additionally, when writing their own code, they tend to rely on returning nulls to indicate something thus requiring the caller to check for nulls.
To put this another way, there are two instances where null checking comes up:

Where null is a valid response in terms of the contract; and

Where it isn't a valid response.


(2) is easy.  As of Java 1.7 you can use Objects.requireNonNull(foo). (If you are stuck with a previous version then assertions may be a good alternative.)
""Proper"" usage of this method would be like below. The method returns the object passed into it and throws a NullPointerException if the object is null. This means that the returned value is always non-null. The method is primarily intended for validating parameters.
public Foo(Bar bar) {
    this.bar = Objects.requireNonNull(bar);
}

It can also be used like an assertion though since it throws an exception if the object is null. In both uses, a message can be added which will be shown in the exception. Below is using it like an assertion and providing a message.
Objects.requireNonNull(someobject, ""if someobject is null then something is wrong"");
someobject.doCalc();

Generally throwing a specific exception like NullPointerException when a value is null but shouldn't be is favorable to throwing a more general exception like AssertionError. This is the approach the Java library takes; favoring NullPointerException over IllegalArgumentException when an argument is not allowed to be null.
(1) is a little harder.  If you have no control over the code you're calling then you're stuck.  If null is a valid response, you have to check for it.
If it's code that you do control, however (and this is often the case), then it's a different story.  Avoid using nulls as a response.  With methods that return collections, it's easy: return empty collections (or arrays) instead of nulls pretty much all the time.
With non-collections it might be harder.  Consider this as an example: if you have these interfaces:
public interface Action {
  void doSomething();
}

public interface Parser {
  Action findAction(String userInput);
}

where Parser takes raw user input and finds something to do, perhaps if you're implementing a command line interface for something.  Now you might make the contract that it returns null if there's no appropriate action.  That leads the null checking you're talking about.
An alternative solution is to never return null and instead use the Null Object pattern:
public class MyParser implements Parser {
  private static Action DO_NOTHING = new Action() {
    public void doSomething() { /* do nothing */ }
  };

  public Action findAction(String userInput) {
    // ...
    if ( /* we can't find any actions */ ) {
      return DO_NOTHING;
    }
  }
}

Compare:
Parser parser = ParserFactory.getParser();
if (parser == null) {
  // now what?
  // this would be an example of where null isn't (or shouldn't be) a valid response
}
Action action = parser.findAction(someInput);
if (action == null) {
  // do nothing
} else {
  action.doSomething();
}

to
ParserFactory.getParser().findAction(someInput).doSomething();

which is a much better design because it leads to more concise code.
That said, perhaps it is entirely appropriate for the findAction() method to throw an Exception with a meaningful error message -- especially in this case where you are relying on user input.  It would be much better for the findAction method to throw an Exception than for the calling method to blow up with a simple NullPointerException with no explanation.
try {
    ParserFactory.getParser().findAction(someInput).doSomething();
} catch(ActionNotFoundException anfe) {
    userConsole.err(anfe.getMessage());
}

Or if you think the try/catch mechanism is too ugly, rather than Do Nothing your default action should provide feedback to the user.
public Action findAction(final String userInput) {
    /* Code to return requested Action if found */
    return new Action() {
        public void doSomething() {
            userConsole.err(""Action not found: "" + userInput);
        }
    }
}

"
271526,2386013,710,"If you use (or planning to use) a Java IDE like JetBrains IntelliJ IDEA, Eclipse or Netbeans or a tool like findbugs then you can use annotations to solve this problem.
Basically, you've got @Nullable and @NotNull.
You can use in method and parameters, like this:
@NotNull public static String helloWorld() {
    return ""Hello World"";
}

or
@Nullable public static String helloWorld() {
    return ""Hello World"";
}

The second example won't compile (in IntelliJ IDEA).
When you use the first helloWorld() function in another piece of code:
public static void main(String[] args)
{
    String result = helloWorld();
    if(result != null) {
        System.out.println(result);
    }
}

Now the IntelliJ IDEA compiler will tell you that the check is useless, since the helloWorld() function won't return null, ever.
Using parameter
void someMethod(@NotNull someParameter) { }

if you write something like:
someMethod(null);

This won't compile.
Last example using @Nullable
@Nullable iWantToDestroyEverything() { return null; }

Doing this
iWantToDestroyEverything().something();

And you can be sure that this won't happen. :)
It's a nice way to let the compiler check something more than it usually does and to enforce your contracts to be stronger. Unfortunately, it's not supported by all the compilers.
In IntelliJ IDEA 10.5 and on, they added support for any other @Nullable @NotNull implementations.
See blog post More flexible and configurable @Nullable/@NotNull annotations.
"
271526,271616,352,"If null-values are not allowed
If your method is called externally, start with something like this:
public void method(Object object) {
  if (object == null) {
    throw new IllegalArgumentException(""..."");
  }

Then, in the rest of that method, you'll know that object is not null.
If it is an internal method (not part of an API), just document that it cannot be null, and that's it.
Example:
public String getFirst3Chars(String text) {
  return text.subString(0, 3);
}

However, if your method just passes the value on, and the next method passes it on etc. it could get problematic. In that case you may want to check the argument as above.
If null is allowed
This really depends. If find that I often do something like this:
if (object == null) {
  // something
} else {
  // something else
}

So I branch, and do two completely different things. There is no ugly code snippet, because I really need to do two different things depending on the data. For example, should I work on the input, or should I calculate a good default value?

It's actually rare for me to use the idiom ""if (object != null && ..."".
It may be easier to give you examples, if you show examples of where you typically use the idiom.
"
271526,452820,257,"Wow, I almost hate to add another answer when we have 57 different ways to recommend the NullObject pattern, but I think that some people interested in this question may like to know that there is a proposal on the table for Java 7 to add ""null-safe handling""—a streamlined syntax for if-not-equal-null logic.
The example given by Alex Miller looks like this:
public String getPostcode(Person person) {  
  return person?.getAddress()?.getPostcode();  
}  

The ?. means only de-reference the left identifier if it is not null, otherwise evaluate the remainder of the expression as null. Some people, like Java Posse member Dick Wall and the voters at Devoxx really love this proposal, but there is opposition too, on the grounds that it will actually encourage more use of null as a sentinel value.

Update: An official proposal for a null-safe operator in Java 7 has been submitted under Project Coin. The syntax is a little different than the example above, but it's the same notion.

Update: The null-safe operator proposal didn't make it into Project Coin. So, you won't be seeing this syntax in Java 7.
"
271526,2064441,212,"If undefined values are not permitted:
You might configure your IDE to warn you about potential null dereferencing. E.g. in Eclipse, see Preferences > Java > Compiler > Errors/Warnings/Null analysis.
If undefined values are permitted:
If you want to define a new API where undefined values make sense, use the Option Pattern (may be familiar from functional languages). It has the following advantages:

It is stated explicitly in the API whether an input or output exists or not.
The compiler forces you to handle the ""undefined"" case.
Option is a monad, so there is no need for verbose null checking, just use map/foreach/getOrElse or a similar combinator to safely use the value (example).

Java 8 has a built-in Optional class (recommended); for earlier versions, there are library alternatives, for example Guava's Optional or FunctionalJava's Option. But like many functional-style patterns, using Option in Java (even 8) results in quite some boilerplate, which you can reduce using a less verbose JVM language, e.g. Scala or Xtend.
If you have to deal with an API which might return nulls, you can't do much in Java. Xtend and Groovy have the Elvis operator ?: and the null-safe dereference operator ?., but note that this returns null in case of a null reference, so it just ""defers"" the proper handling of null.
"
7074,7077,7045,"string is an alias in C# for System.String.
So technically, there is no difference.  It's like int vs. System.Int32.
As far as guidelines, it's generally recommended to use string any time you're referring to an object.
e.g.
string place = ""world"";

Likewise, I think it's generally recommended to use String if you need to refer specifically to the class.
e.g.
string greet = String.Format(""Hello {0}!"", place);

This is the style that Microsoft tends to use in their examples.
It appears that the guidance in this area may have changed, as StyleCop now enforces the use of the C# specific aliases.
"
7074,215422,3799,"
Just for the sake of completeness, here's a brain dump of related information...
As others have noted, string is an alias for System.String. Assuming your code using String compiles to System.String (i.e. you haven't got a using directive for some other namespace with a different String type), they compile to the same code, so at execution time there is no difference whatsoever. This is just one of the aliases in C#. The complete list is:
object:  System.Object
string:  System.String
bool:    System.Boolean
byte:    System.Byte
sbyte:   System.SByte
short:   System.Int16
ushort:  System.UInt16
int:     System.Int32
uint:    System.UInt32
long:    System.Int64
ulong:   System.UInt64
float:   System.Single
double:  System.Double
decimal: System.Decimal
char:    System.Char

Apart from string and object, the aliases are all to value types. decimal is a value type, but not a primitive type in the CLR. The only primitive type which doesn't have an alias is System.IntPtr.
In the spec, the value type aliases are known as ""simple types"". Literals can be used for constant values of every simple type; no other value types have literal forms available. (Compare this with VB, which allows DateTime literals, and has an alias for it too.)
There is one circumstance in which you have to use the aliases: when explicitly specifying an enum's underlying type. For instance:
public enum Foo : UInt32 {} // Invalid
public enum Bar : uint   {} // Valid

That's just a matter of the way the spec defines enum declarations - the part after the colon has to be the integral-type production, which is one token of sbyte, byte, short, ushort, int, uint, long, ulong, char... as opposed to a type production as used by variable declarations for example. It doesn't indicate any other difference.
Finally, when it comes to which to use: personally I use the aliases everywhere for the implementation, but the CLR type for any APIs. It really doesn't matter too much which you use in terms of implementation - consistency among your team is nice, but no-one else is going to care. On the other hand, it's genuinely important that if you refer to a type in an API, you do so in a language-neutral way. A method called ReadInt32 is unambiguous, whereas a method called ReadInt requires interpretation. The caller could be using a language that defines an int alias for Int16, for example. The .NET framework designers have followed this pattern, good examples being in the BitConverter, BinaryReader and Convert classes.
"
7074,215304,829,"String stands for System.String and it is a .NET Framework type. string is an alias in the C# language for  System.String. Both of them are compiled to System.String in IL (Intermediate Language), so there is no difference. Choose what you like and use that. If you code in C#, I'd prefer string as it's a C# type alias and well-known by C# programmers.
I can say the same about (int, System.Int32) etc..
"
7074,12863,591,"The best answer I have ever heard about using the provided type aliases in C# comes from Jeffrey Richter in his book CLR Via C#. Here are his 3 reasons:


I've seen a number of developers confused, not knowing whether to use string or String in their code. Because in C# the string (a keyword) maps exactly to System.String (an FCL type), there is no difference and either can be used.
In C#, long maps to System.Int64, but in a different programming language, long could map to an Int16 or Int32. In fact, C++/CLI does in fact treat long as an Int32. Someone reading source code in one language could easily misinterpret the code's intention if he or she were used to programming in a different programming language. In fact, most languages won't even treat long as a keyword and won't compile code that uses it.
The FCL has many methods that have type names as part of their method names. For example, the BinaryReader type offers methods such as ReadBoolean, ReadInt32, ReadSingle, and so on, and the System.Convert type offers methods such as ToBoolean, ToInt32, ToSingle, and so on. Although it's legal to write the following code, the line with float feels very unnatural to me, and it's not obvious that the line is correct:


BinaryReader br = new BinaryReader(...);
float val  = br.ReadSingle(); // OK, but feels unnatural
Single val = br.ReadSingle(); // OK and feels good

So there you have it. I think these are all really good points. I however, don't find myself using Jeffrey's advice in my own code. Maybe I am too stuck in my C# world but I end up trying to make my code look like the framework code.
"
7074,580546,525,"string is a reserved word, but String is just a class name. 
This means that string cannot be used as a variable name by itself.
If for some reason you wanted a variable called string, you'd see only the first of these compiles:
StringBuilder String = new StringBuilder();  // compiles
StringBuilder string = new StringBuilder();  // doesn't compile 

If you really want a variable name called string you can use @ as a prefix:
StringBuilder @string = new StringBuilder();

Another critical difference: Stack Overflow highlights them differently. 
"
105372,105402,5273,"Update: If you're using .NET 5 or newer, use this solution.
foreach (Suit suit in (Suit[]) Enum.GetValues(typeof(Suit)))
{
}

Note: The cast to (Suit[]) is not strictly necessary, but it does make the code 0.5 ns faster.
"
105372,105421,825,"It looks to me like you really want to print out the names of each enum, rather than the values. In which case Enum.GetNames() seems to be the right approach.
public enum Suits
{
    Spades,
    Hearts,
    Clubs,
    Diamonds,
    NumSuits
}

public void PrintAllSuits()
{
    foreach (string name in Enum.GetNames(typeof(Suits)))
    {
        System.Console.WriteLine(name);
    }
}

By the way, incrementing the value is not a good way to enumerate the values of an enum. You should do this instead.
I would use Enum.GetValues(typeof(Suit)) instead.
public enum Suits
{
    Spades,
    Hearts,
    Clubs,
    Diamonds,
    NumSuits
}

public void PrintAllSuits()
{
    foreach (var suit in Enum.GetValues(typeof(Suits)))
    {
        System.Console.WriteLine(suit.ToString());
    }
}

"
105372,944352,390,"I made some extensions for easy enum usage. Maybe someone can use it...
public static class EnumExtensions
{
    /// <summary>
    /// Gets all items for an enum value.
    /// </summary>
    /// <typeparam name=""T""></typeparam>
    /// <param name=""value"">The value.</param>
    /// <returns></returns>
    public static IEnumerable<T> GetAllItems<T>(this Enum value)
    {
        foreach (object item in Enum.GetValues(typeof(T)))
        {
            yield return (T)item;
        }
    }

    /// <summary>
    /// Gets all items for an enum type.
    /// </summary>
    /// <typeparam name=""T""></typeparam>
    /// <param name=""value"">The value.</param>
    /// <returns></returns>
    public static IEnumerable<T> GetAllItems<T>() where T : struct
    {
        foreach (object item in Enum.GetValues(typeof(T)))
        {
            yield return (T)item;
        }
    }

    /// <summary>
    /// Gets all combined items from an enum value.
    /// </summary>
    /// <typeparam name=""T""></typeparam>
    /// <param name=""value"">The value.</param>
    /// <returns></returns>
    /// <example>
    /// Displays ValueA and ValueB.
    /// <code>
    /// EnumExample dummy = EnumExample.Combi;
    /// foreach (var item in dummy.GetAllSelectedItems<EnumExample>())
    /// {
    ///    Console.WriteLine(item);
    /// }
    /// </code>
    /// </example>
    public static IEnumerable<T> GetAllSelectedItems<T>(this Enum value)
    {
        int valueAsInt = Convert.ToInt32(value, CultureInfo.InvariantCulture);

        foreach (object item in Enum.GetValues(typeof(T)))
        {
            int itemAsInt = Convert.ToInt32(item, CultureInfo.InvariantCulture);

            if (itemAsInt == (valueAsInt & itemAsInt))
            {
                yield return (T)item;
            }
        }
    }

    /// <summary>
    /// Determines whether the enum value contains a specific value.
    /// </summary>
    /// <param name=""value"">The value.</param>
    /// <param name=""request"">The request.</param>
    /// <returns>
    ///     <c>true</c> if value contains the specified value; otherwise, <c>false</c>.
    /// </returns>
    /// <example>
    /// <code>
    /// EnumExample dummy = EnumExample.Combi;
    /// if (dummy.Contains<EnumExample>(EnumExample.ValueA))
    /// {
    ///     Console.WriteLine(""dummy contains EnumExample.ValueA"");
    /// }
    /// </code>
    /// </example>
    public static bool Contains<T>(this Enum value, T request)
    {
        int valueAsInt = Convert.ToInt32(value, CultureInfo.InvariantCulture);
        int requestAsInt = Convert.ToInt32(request, CultureInfo.InvariantCulture);

        if (requestAsInt == (valueAsInt & requestAsInt))
        {
            return true;
        }

        return false;
    }
}

The enum itself must be decorated with the FlagsAttribute:
[Flags]
public enum EnumExample
{
    ValueA = 1,
    ValueB = 2,
    ValueC = 4,
    ValueD = 8,
    Combi = ValueA | ValueB
}

"
105372,1375234,202,"Some versions of the .NET framework do not support Enum.GetValues. Here's a good workaround from Ideas 2.0: Enum.GetValues in Compact Framework:
public Enum[] GetValues(Enum enumeration)
{
    FieldInfo[] fields = enumeration.GetType().GetFields(BindingFlags.Static | BindingFlags.Public);
    Enum[] enumerations = new Enum[fields.Length];

    for (var i = 0; i < fields.Length; i++)
        enumerations[i] = (Enum) fields[i].GetValue(enumeration);

    return enumerations;
}

As with any code that involves reflection, you should take steps to ensure it runs only once and results are cached.
"
105372,65103244,165,"New .NET 5+ Solution:
.NET 5 has introduced a new generic version for the GetValues method:
Suit[] suitValues = Enum.GetValues<Suit>();

Which is now by far the most convenient way of doing this.
Usage in a foreach loop:
foreach (Suit suit in Enum.GetValues<Suit>())
{

}

And if you just need the enum names as strings, you can use the GetNames method instead:
string[] suitNames = Enum.GetNames<Suit>();

"
29482,29485,4644,"From an int:
YourEnum foo = (YourEnum)yourInt;

From a string:
YourEnum foo = (YourEnum) Enum.Parse(typeof(YourEnum), yourString);

// The foo.ToString().Contains("","") check is necessary for 
// enumerations marked with a [Flags] attribute.
if (!Enum.IsDefined(typeof(YourEnum), foo) && !foo.ToString().Contains("",""))
{
    throw new InvalidOperationException(
        $""{yourString} is not an underlying value of the YourEnum enumeration.""
    );
}

From a number:
YourEnum foo = (YourEnum)Enum.ToObject(typeof(YourEnum), yourInt);

"
29482,29489,1152,"Just cast it:
MyEnum e = (MyEnum)3;

Check if it's in range using Enum.IsDefined:
if (Enum.IsDefined(typeof(MyEnum), 3)) { ... }

"
29482,8094628,287,"Alternatively, use an extension method instead of a one-liner:
public static T ToEnum<T>(this string enumString)
{
    return (T) Enum.Parse(typeof (T), enumString);
}

Usage:
Color colorEnum = ""Red"".ToEnum<Color>();

OR
string color = ""Red"";
var colorEnum = color.ToEnum<Color>();

"
29482,22830894,221,"I think to get a complete answer, people have to know how enums work internally in .NET. 
How stuff works
An enum in .NET is a structure that maps a set of values (fields) to a basic type (the default is int). However, you can actually choose the integral type that your enum maps to:
public enum Foo : short

In this case the enum is mapped to the short data type, which means it will be stored in memory as a short and will behave as a short when you cast and use it. 
If you look at it from a IL point of view, a (normal, int) enum looks like this:
.class public auto ansi serializable sealed BarFlag extends System.Enum
{
    .custom instance void System.FlagsAttribute::.ctor()
    .custom instance void ComVisibleAttribute::.ctor(bool) = { bool(true) }

    .field public static literal valuetype BarFlag AllFlags = int32(0x3fff)
    .field public static literal valuetype BarFlag Foo1 = int32(1)
    .field public static literal valuetype BarFlag Foo2 = int32(0x2000)

    // and so on for all flags or enum values

    .field public specialname rtspecialname int32 value__
}

What should get your attention here is that the value__ is stored separately from the enum values. In the case of the enum Foo above, the type of value__ is int16. This basically means that you can store whatever you want in an enum, as long as the types match.
At this point I'd like to point out that System.Enum is a value type, which basically means that BarFlag will take up 4 bytes in memory and Foo will take up 2 -- e.g. the size of the underlying type (it's actually more complicated than that, but hey...).
The answer
So, if you have an integer that you want to map to an enum, the runtime only has to do 2 things: copy the 4 bytes and name it something else (the name of the enum). Copying is implicit because the data is stored as value type - this basically means that if you use unmanaged code, you can simply interchange enums and integers without copying data.
To make it safe, I think it's a best practice to know that the underlying types are the same or implicitly convertible and to ensure the enum values exist (they aren't checked by default!).
To see how this works, try the following code:
public enum MyEnum : int
{
    Foo = 1,
    Bar = 2,
    Mek = 5
}

static void Main(string[] args)
{
    var e1 = (MyEnum)5;
    var e2 = (MyEnum)6;

    Console.WriteLine(""{0} {1}"", e1, e2);
    Console.ReadLine();
}

Note that casting to e2 also works! From the compiler perspective above this makes sense: the value__ field is simply filled with either 5 or 6 and when Console.WriteLine calls ToString(), the name of e1 is resolved while the name of e2 is not.
If that's not what you intended, use Enum.IsDefined(typeof(MyEnum), 6) to check if the value you are casting maps to a defined enum.
Also note that I'm explicit about the underlying type of the enum, even though the compiler actually checks this. I'm doing this to ensure I don't run into any surprises down the road. To see these surprises in action, you can use the following code (actually I've seen this happen a lot in database code):
public enum MyEnum : short
{
    Mek = 5
}

static void Main(string[] args)
{
    var e1 = (MyEnum)32769; // will not compile, out of bounds for a short

    object o = 5;
    var e2 = (MyEnum)o;     // will throw at runtime, because o is of type int

    Console.WriteLine(""{0} {1}"", e1, e2);
    Console.ReadLine();
}

"
29482,29488,143,"Take the following example:
int one = 1;
MyEnum e = (MyEnum)one;

"
444798,444818,3109,"You could use the String.IndexOf Method and pass StringComparison.OrdinalIgnoreCase as the type of search to use:
string title = ""STRING"";
bool contains = title.IndexOf(""string"", StringComparison.OrdinalIgnoreCase) >= 0;

Even better is defining a new extension method for string:
public static class StringExtensions
{
    public static bool Contains(this string source, string toCheck, StringComparison comp)
    {
        return source?.IndexOf(toCheck, comp) >= 0;
    }
}

Note, that null propagation ?. is available since C# 6.0 (VS 2015), for older versions use
if (source == null) return false;
return source.IndexOf(toCheck, comp) >= 0;

USAGE:
string title = ""STRING"";
bool contains = title.Contains(""string"", StringComparison.OrdinalIgnoreCase);

"
444798,15464440,1576,"To test if the string paragraph contains the string word (thanks @QuarterMeister)
culture.CompareInfo.IndexOf(paragraph, word, CompareOptions.IgnoreCase) >= 0

Where culture is the instance of CultureInfo describing the language that the text is written in.
This solution is transparent about the definition of case-insensitivity, which is language dependent. For example, the English language uses the characters I and i for the upper and lower case versions of the ninth letter, whereas the Turkish language uses these characters for the eleventh and twelfth letters of its 29 letter-long alphabet. The Turkish upper case version of 'i' is the unfamiliar character 'İ'.
Thus the strings tin and TIN are the same word in English, but different words in Turkish. As I understand, one means 'spirit' and the other is an onomatopoeia word. (Turks, please correct me if I'm wrong, or suggest a better example)
To summarise, you can only answer the question 'are these two strings the same but in different cases' if you know what language the text is in. If you don't know, you'll have to take a punt. Given English's hegemony in software, you should probably resort to CultureInfo.InvariantCulture, because it will be wrong in familiar ways.
"
444798,444834,275,"You can use IndexOf() like this:
string title = ""STRING"";

if (title.IndexOf(""string"", 0, StringComparison.OrdinalIgnoreCase) != -1)
{
    // The string exists in the original
}

Since 0 (zero) can be an index, you check against -1.
Microsoft .NET Documentation:

The zero-based index position of the value parameter from the start of the current instance if that string is found, or -1 if it is not. If value is Empty, the return value is startIndex.

"
444798,52791476,205,".NET Core 2.0+ (including .NET 5.0+)
.NET Core has had a pair of methods to deal with this since version 2.0 :

String.Contains(Char, StringComparison)
String.Contains(String, StringComparison)

Example:
""Test"".Contains(""test"", System.StringComparison.CurrentCultureIgnoreCase);

It is now officially part of the .NET Standard 2.1, and therefore part of all the implementations of the Base Class Library that implement this version of the standard (or a higher one).
"
444798,3355561,158,"Alternative solution using Regex:
bool contains = Regex.IsMatch(""StRiNG to search"", Regex.Escape(""string""), RegexOptions.IgnoreCase);

"
141088,141098,4503,"foreach(KeyValuePair<string, string> entry in myDictionary)
{
    // do something with entry.Value or entry.Key
}

"
141088,141105,1113,"If you are trying to use a generic Dictionary in C# like you would use an associative array in another language:
foreach(var item in myDictionary)
{
  foo(item.Key);
  bar(item.Value);
}

Or, if you only need to iterate over the collection of keys, use
foreach(var item in myDictionary.Keys)
{
  foo(item);
}

And lastly, if you're only interested in the values:
foreach(var item in myDictionary.Values)
{
  foo(item);
}

(Take note that the var keyword is an optional C# 3.0 and above feature, you could also use the exact type of your keys/values here)
"
141088,5265690,200,"In some cases you may need a counter that may be provided by for-loop implementation. For that, LINQ provides ElementAt which enables the following:
for (int index = 0; index < dictionary.Count; index++) {
  var item = dictionary.ElementAt(index);
  var itemKey = item.Key;
  var itemValue = item.Value;
}

"
141088,141132,122,"Depends on whether you're after the keys or the values...
From the MSDN Dictionary(TKey, TValue) Class description:
// When you use foreach to enumerate dictionary elements,
// the elements are retrieved as KeyValuePair objects.
Console.WriteLine();
foreach( KeyValuePair<string, string> kvp in openWith )
{
    Console.WriteLine(""Key = {0}, Value = {1}"", 
        kvp.Key, kvp.Value);
}

// To get the values alone, use the Values property.
Dictionary<string, string>.ValueCollection valueColl =
    openWith.Values;

// The elements of the ValueCollection are strongly typed
// with the type that was specified for dictionary values.
Console.WriteLine();
foreach( string s in valueColl )
{
    Console.WriteLine(""Value = {0}"", s);
}

// To get the keys alone, use the Keys property.
Dictionary<string, string>.KeyCollection keyColl =
    openWith.Keys;

// The elements of the KeyCollection are strongly typed
// with the type that was specified for dictionary keys.
Console.WriteLine();
foreach( string s in keyColl )
{
    Console.WriteLine(""Key = {0}"", s);
}

"
141088,31918117,98,"Generally, asking for ""the best way"" without a specific context is like asking 
what is the best color?
One the one hand, there are many colors and there's no best color.  It depends on the need and often on taste, too.
On the other hand, there are many ways to iterate over a Dictionary in C# and there's no best way. It depends on the need and often on taste, too.
Most straightforward way
foreach (var kvp in items)
{
    // key is kvp.Key
    doStuff(kvp.Value)
}

If you need only the value (allows to call it item, more readable than kvp.Value).
foreach (var item in items.Values)
{
    doStuff(item)
}

If you need a specific sort order
Generally, beginners are surprised about order of enumeration of a Dictionary.
LINQ provides a concise syntax that allows to specify order (and many other things), e.g.:
foreach (var kvp in items.OrderBy(kvp => kvp.Key))
{
    // key is kvp.Key
    doStuff(kvp.Value)
}

Again you might only need the value. LINQ also provides a concise solution to: 

iterate directly on the value (allows to call it item, more readable than kvp.Value)
but sorted by the keys

Here it is:
foreach (var item in items.OrderBy(kvp => kvp.Key).Select(kvp => kvp.Value))
{
    doStuff(item)
}

There are many more real-world use case you can do from these examples.
If you don't need a specific order, just stick to the ""most straightforward way"" (see above)!
"
3737139,3737177,1311,"Incrementing / Decrementing Operators
++ increment operator
-- decrement operator
Example    Name              Effect
---------------------------------------------------------------------
++$a       Pre-increment     Increments $a by one, then returns $a.
$a++       Post-increment    Returns $a, then increments $a by one.
--$a       Pre-decrement     Decrements $a by one, then returns $a.
$a--       Post-decrement    Returns $a, then decrements $a by one.

These can go before or after the variable.
If put before the variable, the increment/decrement operation is done to the variable first then the result is returned. If put after the variable, the variable is first returned, then the increment/decrement operation is done.
For example:
$apples = 10;
for ($i = 0; $i < 10; ++$i) {
    echo 'I have ' . $apples-- . "" apples. I just ate one.\n"";
}

Live example
In the case above ++$i is used, since it is faster. $i++ would have the same results.
Pre-increment is a little bit faster because it really increments the variable and after that 'returns' the result. Post-increment creates a special variable, copies there the value of the first variable and only after the first variable is used, replaces its value with second's.
However, you must use $apples--, since first, you want to display the current number of apples, and then you want to subtract one from it.
You can also increment letters in PHP:
$i = ""a"";
while ($i < ""c"") {
    echo $i++;
}

Once z is reached aa is next, and so on.

Note that character variables can be incremented but not decremented and even so only plain ASCII characters (a-z and A-Z) are supported.


Stack Overflow Posts:

Understanding Incrementing 

"
3737139,13159111,495,"Bitwise Operator
What is a bit? A bit is a representation of 1 or 0. Basically OFF(0) and ON(1)
What is a byte? A byte is made up of 8 bits and the highest value of a byte is 255, which would mean every bit is set. We will look at why a byte's maximum value is 255.
-------------------------------------------
|      1 Byte ( 8 bits )                  |
-------------------------------------------
|Place Value | 128| 64| 32| 16| 8| 4| 2| 1|     
-------------------------------------------

This representation of 1 Byte
1 + 2 + 4 + 8 + 16 + 32 + 64 + 128 = 255 (1 Byte) 
A few examples for better understanding
The ""AND"" operator: &
$a =  9;
$b = 10;
echo $a & $b;

This would output the number 8. Why? Well let's see using our table example.
-------------------------------------------
|      1 Byte ( 8 bits )                  |
-------------------------------------------
|Place Value | 128| 64| 32| 16| 8| 4| 2| 1|     
-------------------------------------------
|      $a    |   0|  0|  0|  0| 1| 0| 0| 1|    
-------------------------------------------
|      $b    |   0|  0|  0|  0| 1| 0| 1| 0|
------------------------------------------- 
|      &     |   0|  0|  0|  0| 1| 0| 0| 0|
------------------------------------------- 

So you can see from the table the only bit they share together is the 8 bit.
Second example
$a =  36;
$b = 103;
echo $a & $b; // This would output the number 36.
$a = 00100100
$b = 01100111

The two shared bits are 32 and 4, which when added together return 36.
The ""Or"" operator: |
$a =  9;
$b = 10;
echo $a | $b;

This would output the number 11. Why?
-------------------------------------------
|      1 Byte ( 8 bits )                  |
-------------------------------------------
|Place Value | 128| 64| 32| 16| 8| 4| 2| 1|     
-------------------------------------------
|      $a    |   0|  0|  0|  0| 1| 0| 0| 1|    
-------------------------------------------
|      $b    |   0|  0|  0|  0| 1| 0| 1| 0|
------------------------------------------- 
|      |     |   0|  0|  0|  0| 1| 0| 1| 1|
-------------------------------------------

You will notice that we have 3 bits set, in the 8, 2, and 1 columns. Add those up: 8+2+1=11.
"
3737139,31298778,371,"<=> Spaceship Operator
Added in PHP 7
The spaceship operator <=> is the latest comparison operator added in PHP 7. It is a non-associative binary operator with the same precedence as equality operators (==, !=, ===, !==). This operator allows for simpler three-way comparison between left-hand and right-hand operands.
The operator results in an integer expression of:

0 when both operands are equal
Less than 0 when the left-hand operand is less than the right-hand operand
Greater than 0 when the left-hand operand is greater than the right-hand operand

e.g.
1 <=> 1; // 0
1 <=> 2; // -1
2 <=> 1; // 1

A good practical application of using this operator would be in comparison type callbacks that are expected to return a zero, negative, or positive integer based on a three-way comparison between two values. The comparison function passed to usort is one such example.
Before PHP 7 you would write...
$arr = [4,2,1,3];

usort($arr, function ($a, $b) {
    if ($a < $b) {
        return -1;
    } elseif ($a > $b) {
        return 1;
    } else {
        return 0;
    }
});

Since PHP 7 you can write...
$arr = [4,2,1,3];

usort($arr, function ($a, $b) {
    return $a <=> $b;
    // return $b <=> $a; // for reversing order
});

"
3737139,13622199,315,"_ Alias for gettext() 
The underscore character '_' as in _() is an alias to the gettext() function.
"
3737139,13158520,311,"



Syntax
Name
Description




x == y
Equality
true if x and y have the same key/value pairs


x != y
Inequality
true if x is not equal to y


x === y
Identity
true if x and y have the same key/value pairsin the same order and of the same types


x !== y
Non-identity
true if x is not identical to y


x <=> y
Spaceship
0 if x is equal to y, greater than 0 if x > y, less than 0 if x < y


++x
Pre-increment
Increments x by one, then returns x


x++
Post-increment
Returns x, then increments x by one


--x
Pre-decrement
Decrements x by one, then returns x


x--
Post-decrement
Returns x, then decrements x by one


x and y
And
true if both x and y are true. If x=6, y=3 then(x < 10 and y > 1) returns true


x && y
And
true if both x and y are true. If x=6, y=3 then(x < 10 && y > 1) returns true


x or y
Or
true if any of x or y are true. If x=6, y=3 then(x < 10 or y > 10) returns true


x || y
Or
true if any of x or y are true. If x=6, y=3 then(x < 3 || y > 1) returns true


a . b
Concatenation
Concatenate two strings: ""Hi"" . ""Ha""



"
369602,369608,3578,"There are different ways to delete an array element, where some are more useful for some specific tasks than others.
Deleting a Single Array Element
If you want to delete just one single array element you can use unset() and alternatively array_splice().
By key or by value?
If you know the value and don't know the key to delete the element you can use array_search() to get the key.
This only works if the element doesn't occur more than once, since array_search() returns the first hit only.
unset() Expression
Note: When you use unset() the array keys won’t change.
If you want to reindex the keys you can use array_values() after unset(),
which will convert all keys to numerically enumerated keys starting from 0
(the array remains a list).
Example Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
unset($array[1]);
          // ↑ Key of element to delete

Example Output:
[
    [0] => a
    [2] => c
]

array_splice() Function
If you use array_splice() the (integer) keys will automatically be reindex-ed,
but the associative (string) keys won't change — as opposed to array_values() after unset(),
which will convert all keys to numerical keys.
Note: array_splice()
needs the offset, not the key, as the second parameter; offset = array_flip(array_keys(array))[key].
Example Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
array_splice($array, 1, 1);
                  // ↑ Offset of element to delete

Example Output:
[
    [0] => a
    [1] => c
]

array_splice(), same as unset(), take the array by reference. You don’t assign the return values back to the array.
Deleting Multiple Array Elements
If you want to delete multiple array elements and don’t want
to call unset() or array_splice() multiple times you can use the functions array_diff() or
array_diff_key() depending on whether you know the values or the keys of the elements to remove from the array.
array_diff() Function
If you know the values of the array elements which you want to delete, then you can use array_diff().
As before with unset() it won’t change the keys of the array.
Example Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c"", 3 => ""c""];
$array = array_diff($array, [""a"", ""c""]);
                         // └────────┘
                         // Array values to delete

Example Output:
[
    [1] => b
]

array_diff_key() Function
If you know the keys of the elements which you want to delete, then you want to use array_diff_key().
You have to make sure you pass the keys as keys in the second parameter and not as values.
Keys won’t reindex.
Example Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
$array = array_diff_key($array, [0 => ""xy"", ""2"" => ""xy""]);
                              // ↑           ↑
                              // Array keys of elements to delete

Example Output:
[
    [1] => b
]

If you want to use unset() or array_splice() to delete multiple elements with the same value you can use
array_keys() to get all the keys for a specific value
and then delete all elements.
array_filter() Function
If you want to delete all elements with a specific value in the array you can use array_filter().
Example Code:
$array = [0 => ""a"", 1 => ""b"", 2 => ""c""];
$array = array_filter($array, static function ($element) {
    return $element !== ""b"";
    //                   ↑
    // Array value which you want to delete
});

Example Output:
[
    [0] => a
    [2] => c
]

"
369602,369761,1423,"It should be noted that unset() will keep indexes untouched, which is what you'd expect when using string indexes (array as hashtable), but can be quite surprising when dealing with integer indexed arrays:
$array = array(0, 1, 2, 3);
unset($array[2]);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [3]=>
  int(3)
} */

$array = array(0, 1, 2, 3);
array_splice($array, 2, 1);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [2]=>
  int(3)
} */

So array_splice() can be used if you'd like to normalize your integer keys. Another option is using array_values() after unset():
$array = array(0, 1, 2, 3);

unset($array[2]);
$array = array_values($array);
var_dump($array);
/* array(3) {
  [0]=>
  int(0)
  [1]=>
  int(1)
  [2]=>
  int(3)
} */

"
369602,6914929,426,"  // Our initial array
  $arr = array(""blue"", ""green"", ""red"", ""yellow"", ""green"", ""orange"", ""yellow"", ""indigo"", ""red"");
  print_r($arr);

  // Remove the elements who's values are yellow or red
  $arr = array_diff($arr, array(""yellow"", ""red""));
  print_r($arr);

This is the output from the code above:
Array
(
    [0] => blue
    [1] => green
    [2] => red
    [3] => yellow
    [4] => green
    [5] => orange
    [6] => yellow
    [7] => indigo
    [8] => red
)

Array
(
    [0] => blue
    [1] => green
    [4] => green
    [5] => orange
    [7] => indigo
)

Now, array_values() will reindex a numerical array nicely, but it will remove all key strings from the array and replace them with numbers. If you need to preserve the key names (strings), or reindex the array if all keys are numerical, use array_merge():
$arr = array_merge(array_diff($arr, array(""yellow"", ""red"")));
print_r($arr);

Outputs
Array
(
    [0] => blue
    [1] => green
    [2] => green
    [3] => orange
    [4] => indigo
)

"
369602,8135667,242,"$key = array_search($needle, $array);
if ($key !== false) {
    unset($array[$key]);
}

"
369602,369612,109,"unset($array[$index]);

"
2068344,2068371,5356,"
Each YouTube video has four generated images. They are predictably formatted as follows:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg
https://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg

The first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg

For the high quality version of the thumbnail use a URL similar to this:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg

There is also a medium quality version of the thumbnail, using a URL similar to the HQ:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg

For the standard definition version of the thumbnail, use a URL similar to this:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg

For the maximum resolution version of the thumbnail use a URL similar to this:
https://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg

All of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.
Alternatively, you can use the YouTube Data API (v3) to get thumbnail images.
"
2068344,2108248,433,"You can use YouTube Data API to retrieve video thumbnails, caption, description, rating, statistics and more. API version 3 requires a key*. Obtain the key and create a videos: list request:
https://www.googleapis.com/youtube/v3/videos?key=YOUR_API_KEY&part=snippet&id=VIDEO_ID

Example PHP Code
$data = file_get_contents(""https://www.googleapis.com/youtube/v3/videos?key=YOUR_API_KEY&part=snippet&id=T0Jqdjbed40"");
$json = json_decode($data);
var_dump($json->items[0]->snippet->thumbnails);

Output
object(stdClass)#5 (5) {
  [""default""]=>
  object(stdClass)#6 (3) {
    [""url""]=>
    string(46) ""https://i.ytimg.com/vi/T0Jqdjbed40/default.jpg""
    [""width""]=>
    int(120)
    [""height""]=>
    int(90)
  }
  [""medium""]=>
  object(stdClass)#7 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/mqdefault.jpg""
    [""width""]=>
    int(320)
    [""height""]=>
    int(180)
  }
  [""high""]=>
  object(stdClass)#8 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/hqdefault.jpg""
    [""width""]=>
    int(480)
    [""height""]=>
    int(360)
  }
  [""standard""]=>
  object(stdClass)#9 (3) {
    [""url""]=>
    string(48) ""https://i.ytimg.com/vi/T0Jqdjbed40/sddefault.jpg""
    [""width""]=>
    int(640)
    [""height""]=>
    int(480)
  }
  [""maxres""]=>
  object(stdClass)#10 (3) {
    [""url""]=>
    string(52) ""https://i.ytimg.com/vi/T0Jqdjbed40/maxresdefault.jpg""
    [""width""]=>
    int(1280)
    [""height""]=>
    int(720)
  }
}

* Not only that you need a key, you might be asked for billing information depending on the number of API requests you plan to make. However, few thousand requests per day are free.
Source article.
"
2068344,20542029,349,"What Asaph said is right. However, not every YouTube video contains all nine
thumbnails. Also, the thumbnails' image sizes depends on the video (the numbers
below are based on one). There are some thumbnails guaranteed to exist:




Width
Height
URL




120
90
https://i.ytimg.com/vi//1.jpg


120
90
https://i.ytimg.com/vi//2.jpg


120
90
https://i.ytimg.com/vi//3.jpg


120
90
https://i.ytimg.com/vi//default.jpg


320
180
https://i.ytimg.com/vi//mq1.jpg


320
180
https://i.ytimg.com/vi//mq2.jpg


320
180
https://i.ytimg.com/vi//mq3.jpg


320
180
https://i.ytimg.com/vi//mqdefault.jpg


480
360
https://i.ytimg.com/vi//0.jpg


480
360
https://i.ytimg.com/vi//hq1.jpg


480
360
https://i.ytimg.com/vi//hq2.jpg


480
360
https://i.ytimg.com/vi//hq3.jpg


480
360
https://i.ytimg.com/vi//hqdefault.jpg




Additionally, the some other thumbnails may or may not exist. Their presence is
probably based on whether the video is high-quality.




Width
Height
URL




640
480
https://i.ytimg.com/vi//sd1.jpg


640
480
https://i.ytimg.com/vi//sd2.jpg


640
480
https://i.ytimg.com/vi//sd3.jpg


640
480
https://i.ytimg.com/vi//sddefault.jpg


1280
720
https://i.ytimg.com/vi//hq720.jpg


1920
1080
https://i.ytimg.com/vi//maxresdefault.jpg




You can find JavaScript and PHP scripts to retrieve thumbnails and other
YouTube information in:

How to get YouTube Video Info with PHP
Retrieve YouTube Video Details using JavaScript - JSON & API v2

You can also use the YouTube Video Information Generator tool to get all
the information about a YouTube video by submitting a URL or video id.
"
2068344,18400445,94,"In YouTube API V3 we can also use these URLs for obtaining thumbnails... They are classified based on their quality.
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/default.jpg -   default
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg - medium 
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg - high
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/sddefault.jpg - standard

And for the maximum resolution..
https://i1.ytimg.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg

One advantage of these URLs over the URLs in the first answer is that these URLs don't get blocked by firewalls.
"
2068344,30708608,66,"If you want to get rid of the ""black bars"" and do it like YouTube does it, you can use:
https://i.ytimg.com/vi_webp/<video id>/mqdefault.webp

And if you can't use the .webp file extension you can do it like this:
https://i.ytimg.com/vi/<video id>/mqdefault.jpg

Also, if you need the unscaled version, use maxresdefault instead of mqdefault.
Note: I'm not sure about the aspect ratio if you're planning to use maxresdefault.
"
60174,60496,9640,"The correct way to avoid SQL injection attacks, no matter which database you use, is to separate the data from SQL, so that data stays data and will never be interpreted as commands by the SQL parser. It is possible to create an SQL statement with correctly formatted data parts, but if you don't fully understand the details, you should always use prepared statements and parameterized queries. These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.
You basically have two options to achieve this:

Using PDO (for any supported database driver):
$stmt = $pdo->prepare('SELECT * FROM employees WHERE name = :name');
$stmt->execute([ 'name' => $name ]);

foreach ($stmt as $row) {
    // Do something with $row
}


Using MySQLi (for MySQL):
Since PHP 8.2+ we can make use of execute_query() which prepares, binds parameters, and executes SQL statement in one method:
$result = $db->execute_query('SELECT * FROM employees WHERE name = ?', [$name]);
 while ($row = $result->fetch_assoc()) {
     // Do something with $row
 }

Up to PHP8.1:
 $stmt = $db->prepare('SELECT * FROM employees WHERE name = ?');
 $stmt->bind_param('s', $name); // 's' specifies the variable type => 'string'
 $stmt->execute();
 $result = $stmt->get_result();
 while ($row = $result->fetch_assoc()) {
     // Do something with $row
 }



If you're connecting to a database other than MySQL, there is a driver-specific second option that you can refer to (for example, pg_prepare() and pg_execute() for PostgreSQL). PDO is the universal option.

Correctly setting up the connection
PDO
Note that when using PDO to access a MySQL database real prepared statements are not used by default. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:
$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8mb4', 'user', 'password');

$dbConnection->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);

In the above example, the error mode isn't strictly necessary, but it is advised to add it. This way PDO will inform you of all MySQL errors by means of throwing the PDOException.
What is mandatory, however, is the first setAttribute() line, which tells PDO to disable emulated prepared statements and use real prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).
Although you can set the charset in the options of the constructor, it's important to note that 'older' versions of PHP (before 5.3.6) silently ignored the charset parameter in the DSN.
Mysqli
For mysqli we have to follow the same routine:
mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT); // error reporting
$dbConnection = new mysqli('127.0.0.1', 'username', 'password', 'test');
$dbConnection->set_charset('utf8mb4'); // charset


Explanation
The SQL statement you pass to prepare is parsed and compiled by the database server. By specifying parameters (either a ? or a named parameter like :name in the example above) you tell the database engine where you want to filter on. Then when you call execute, the prepared statement is combined with the parameter values you specify.
The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend.
Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the $name variable contains 'Sarah'; DELETE FROM employees the result would simply be a search for the string ""'Sarah'; DELETE FROM employees"", and you will not end up with an empty table.
Another benefit of using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.
Oh, and since you asked about how to do it for an insert, here's an example (using PDO):
$preparedStatement = $db->prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement->execute([ 'column' => $unsafeValue ]);


Can prepared statements be used for dynamic queries?
While you can still use prepared statements for the query parameters, the structure of the dynamic query itself cannot be parametrized and certain query features cannot be parametrized.
For these specific scenarios, the best thing to do is use a whitelist filter that restricts the possible values.
// Value whitelist
// $dir can only be 'DESC', otherwise it will be 'ASC'
if (empty($dir) || $dir !== 'DESC') {
   $dir = 'ASC';
}

"
60174,60442,1744,"To use the parameterized query, you need to use either Mysqli or PDO. To rewrite your example with mysqli, we would need something like the following.
<?php
mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT);
$mysqli = new mysqli(""server"", ""username"", ""password"", ""database_name"");

$variable = $_POST[""user-input""];
$stmt = $mysqli->prepare(""INSERT INTO table (column) VALUES (?)"");
// ""s"" means the database expects a string
$stmt->bind_param(""s"", $variable);
$stmt->execute();

The key function you'll want to read up on there would be mysqli::prepare.
Also, as others have suggested, you may find it useful/easier to step up a layer of abstraction with something like PDO.
Please note that the case you asked about is a fairly simple one and that more complex cases may require more complex approaches. In particular:

If you want to alter the structure of the SQL based on user input, parameterized queries are not going to help, and the escaping required is not covered by mysql_real_escape_string. In this kind of case, you would be better off passing the user's input through a whitelist to ensure only 'safe' values are allowed through.

"
60174,8255054,1177,"Every answer here covers only part of the problem.
In fact, there are four different query parts which we can add to SQL dynamically: -

a string
a number
an identifier
a syntax keyword

And prepared statements cover only two of them.
But sometimes we have to make our query even more dynamic, adding operators or identifiers as well.
So, we will need different protection techniques.
In general, such a protection approach is based on whitelisting.
In this case, every dynamic parameter should be hardcoded in your script and chosen from that set.
For example, to do dynamic ordering:
$orders  = array(""name"", ""price"", ""qty""); // Field names
$key = array_search($_GET['sort'], $orders)); // if we have such a name
$orderby = $orders[$key]; // If not, first one will be set automatically. 
$query = ""SELECT * FROM `table` ORDER BY $orderby""; // Value is safe

To ease the process I wrote a whitelist helper function that does all the job in one line:
$orderby = white_list($_GET['orderby'], ""name"", [""name"",""price"",""qty""], ""Invalid field name"");
$query  = ""SELECT * FROM `table` ORDER BY `$orderby`""; // sound and safe

There is another way to secure identifiers - escaping but I rather stick to whitelisting as a more robust and explicit approach. Yet as long as you have an identifier quoted, you can escape the quote character to make it safe. For example, by default for mysql you have to double the quote character to escape it. For other other DBMS escaping rules would be different.
Still, there is an issue with SQL syntax keywords (such as AND, DESC and such), but white-listing seems the only approach in this case.
So, a general recommendation may be phrased as


Any variable that represents an SQL data literal, (or, to put it simply - an SQL string, or a number) must be added through a prepared statement. No Exceptions.
Any other query part, such as an SQL keyword, a table or a field name, or an operator - must be filtered through a white list.


Update
Although there is a general agreement on the best practices regarding SQL injection protection, there are still many bad practices as well. And some of them too deeply rooted in the minds of PHP users. For instance, on this very page there are (although invisible to most visitors) more than 80 deleted answers - all removed by the community due to bad quality or promoting bad and outdated practices. Worse yet, some of the bad answers aren't deleted, but rather prospering.
For example, there(1) are(2) still(3) many(4) answers(5), including the second most upvoted answer suggesting you manual string escaping - an outdated approach that is proven to be insecure.
Or there is a slightly better answer that suggests just another method of string formatting and even boasts it as the ultimate panacea. While of course, it is not. This method is no better than regular string formatting, yet it keeps all its drawbacks: it is applicable to strings only and, like any other manual formatting, it's essentially optional, non-obligatory measure, prone to human error of any sort.
I think that all this because of one very old superstition, supported by such authorities like OWASP or the PHP manual, which proclaims equality between whatever ""escaping"" and protection from SQL injections.
Regardless of what PHP manual said for ages, *_escape_string by no means makes data safe and never has been intended to. Besides being useless for any SQL part other than string, manual escaping is wrong, because it is manual as opposite to automated.
And OWASP makes it even worse, stressing on escaping user input which is an utter nonsense: there should be no such words in the context of injection protection. Every variable is potentially dangerous - no matter the source! Or, in other words - every variable has to be properly formatted to be put into a query - no matter the source again. It's the destination that matters. The moment a developer starts to separate the sheep from the goats (thinking whether some particular variable is ""safe"" or not) he/she takes his/her first step towards disaster. Not to mention that even the wording suggests bulk escaping at the entry point, resembling the very magic quotes feature - already despised, deprecated and removed.
So, unlike whatever ""escaping"", prepared statements is the measure that indeed protects from SQL injection (when applicable).
"
60174,60180,901,"I'd recommend using PDO (PHP Data Objects) to run parameterized SQL queries.
Not only does this protect against SQL injection, but it also speeds up queries.
And by using PDO rather than mysql_, mysqli_, and pgsql_ functions, you make your application a little more abstracted from the database, in the rare occurrence that you have to switch database providers.
"
60174,60530,668,"Use PDO and prepared queries.
($conn is a PDO object)
$stmt = $conn->prepare(""INSERT INTO tbl VALUES(:id, :name)"");
$stmt->bindValue(':id', $id);
$stmt->bindValue(':name', $name);
$stmt->execute();

"
12859942,12860046,2231,"The MySQL extension:

Is not under active development
Is officially deprecated as of PHP 5.5 (released June 2013).
Has been removed entirely as of PHP 7.0 (released December 2015)


This means that as of 31 Dec 2018 it does not exist in any supported version of PHP. If you are using a version of PHP which supports it, you are using a version which doesn't get security problems fixed.

Lacks an OO interface
Doesn't support:


Non-blocking, asynchronous queries
Prepared statements or parameterized queries
Stored procedures
Multiple Statements
Transactions
The ""new"" password authentication method (on by default in MySQL 5.6; required in 5.7)
Any of the new functionality in MySQL 5.1 or later


Since it is deprecated, using it makes your code less future proof. 
Lack of support for prepared statements is particularly important as they provide a clearer, less error-prone method of escaping and quoting external data than manually escaping it with a separate function call.
See the comparison of SQL extensions.
"
12859942,14110189,1361,"PHP offers three different APIs to connect to MySQL. These are the mysql(removed as of PHP 7), mysqli, and PDO extensions.
The mysql_* functions used to be very popular, but their use is not encouraged anymore. The documentation team is discussing the database security situation, and educating users to move away from the commonly used ext/mysql extension is part of this (check php.internals: deprecating ext/mysql).
And the later PHP developer team has taken the decision to generate E_DEPRECATED errors when users connect to MySQL, whether through mysql_connect(), mysql_pconnect() or the implicit connection functionality built into ext/mysql.
ext/mysql was officially deprecated as of PHP 5.5 and has been removed as of PHP 7.
See the Red Box?
When you go on any mysql_* function manual page, you see a red box, explaining it should not be used anymore.
Why

Moving away from ext/mysql is not only about security, but also about having access to all the features of the MySQL database.
ext/mysql was built for MySQL 3.23 and only got very few additions since then while mostly keeping compatibility with this old version which makes the code a bit harder to maintain. Missing features that is not supported by ext/mysql include: (from PHP manual).

Stored procedures (can't handle multiple result sets)
Prepared statements
Encryption (SSL)
Compression
Full Charset support

Reason to not use mysql_* function:

Not under active development
Removed as of PHP 7
Lacks an OO interface
Doesn't support non-blocking, asynchronous queries
Doesn't support prepared statements or parameterized queries
Doesn't support stored procedures
Doesn't support multiple statements
Doesn't support transactions
Doesn't support all of the functionality in MySQL 5.1

Above point quoted from Quentin's answer
Lack of support for prepared statements is particularly important as they provide a clearer, less error prone method of escaping and quoting external data than manually escaping it with a separate function call.
See the comparison of SQL extensions.

Suppressing deprecation warnings
While code is being converted to MySQLi/PDO, E_DEPRECATED errors can be suppressed by setting error_reporting in php.ini to exclude E_DEPRECATED:
error_reporting = E_ALL ^ E_DEPRECATED

Note that this will also hide other deprecation warnings, which, however, may be for things other than MySQL. (from PHP manual)
The article PDO vs. MySQLi: Which Should You Use? by Dejan Marjanovic will help you to choose.
And a better way is PDO, and I am now writing a simple PDO tutorial.

A simple and short PDO tutorial

Q. First question in my mind was: what is `PDO`?
A. “PDO – PHP Data Objects – is a database access layer providing a uniform method of access to multiple databases.”




Connecting to MySQL 
With mysql_* function or we can say it the old way (deprecated in PHP 5.5 and above)
$link = mysql_connect('localhost', 'user', 'pass');
mysql_select_db('testdb', $link);
mysql_set_charset('UTF-8', $link);

With PDO: All you need to do is create a new PDO object. The constructor accepts parameters for specifying the database source PDO's constructor mostly takes four parameters which are DSN (data source name) and  optionally username, password.
Here I think you are familiar with all except DSN; this is new in PDO. A DSN is basically a string of options that tell PDO which driver to use, and connection details. For further reference, check PDO MySQL DSN.
$db = new PDO('mysql:host=localhost;dbname=testdb;charset=utf8', 'username', 'password');

Note: you can also use charset=UTF-8, but sometimes it causes an error, so it's better to use utf8.
If there is any connection error, it will throw a PDOException object that can be caught to handle Exception further.
Good read: Connections and Connection management ¶
You can also pass in several driver options as an array to the fourth parameter. I recommend passing the parameter which puts PDO into exception mode. Because some PDO drivers don't support native prepared statements, so PDO performs emulation of the prepare. It also lets you manually enable this emulation. To use the native server-side prepared statements, you should explicitly set it false.
The other is to turn off prepare emulation which is enabled in the MySQL driver by default, but prepare emulation should be turned off to use PDO safely.
I will later explain why prepare emulation should be turned off. To find reason please check this post.
It is only usable if you are using an old version of MySQL which I do not recommended.
Below is an example of how you can do it:
$db = new PDO('mysql:host=localhost;dbname=testdb;charset=UTF-8', 
              'username', 
              'password',
              array(PDO::ATTR_EMULATE_PREPARES => false,
              PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION));

Can we set attributes after PDO construction?
Yes, we can also set some attributes after PDO construction with the setAttribute method:
$db = new PDO('mysql:host=localhost;dbname=testdb;charset=UTF-8', 
              'username', 
              'password');
$db->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
$db->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);

Error Handling 

Error handling is much easier in PDO than mysql_*.
A common practice when using mysql_* is:
//Connected to MySQL
$result = mysql_query(""SELECT * FROM table"", $link) or die(mysql_error($link));

OR die() is not a good way to handle the error since we can not handle the thing in die. It will just end the script abruptly and then echo the error to the screen which you usually do NOT want to show to your end users, and let bloody hackers discover your schema. Alternately, the return values of mysql_* functions can often be used in conjunction with mysql_error() to handle errors.
PDO offers a better solution: exceptions. Anything we do with PDO should be wrapped in a try-catch block. We can force PDO into one of three error modes by setting the error mode attribute. Three error handling modes are below.

PDO::ERRMODE_SILENT. It's just setting error codes and acts pretty much the same as mysql_* where you must check each result and then look at $db->errorInfo(); to get the error details.
PDO::ERRMODE_WARNING Raise E_WARNING. (Run-time warnings (non-fatal errors). Execution of the script is not halted.)
PDO::ERRMODE_EXCEPTION: Throw exceptions. It represents an error raised by PDO. You should not throw a PDOException from your own code. See Exceptions for more information about exceptions in PHP. It acts very much like or die(mysql_error());, when it isn't caught. But unlike or die(), the PDOException can be caught and handled gracefully if you choose to do so.

Good read:

Errors and error handling ¶
The PDOException class ¶
Exceptions ¶

Like:
$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_SILENT );
$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_WARNING );
$stmt->setAttribute( PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION );

And you can wrap it in try-catch, like below:
try {
    //Connect as appropriate as above
    $db->query('hi'); //Invalid query!
} 
catch (PDOException $ex) {
    echo ""An Error occured!""; //User friendly message/message you want to show to user
    some_logging_function($ex->getMessage());
}

You do not have to handle with try-catch right now. You can catch it at any time appropriate, but I strongly recommend you to use try-catch. Also it may make more sense to catch it at outside the function that calls the PDO stuff:
function data_fun($db) {
    $stmt = $db->query(""SELECT * FROM table"");
    return $stmt->fetchAll(PDO::FETCH_ASSOC);
}

//Then later
try {
    data_fun($db);
}
catch(PDOException $ex) {
    //Here you can handle error and show message/perform action you want.
}

Also, you can handle by or die() or we can say like mysql_*, but it will be really varied. You can hide the dangerous error messages in production by turning display_errors off and just reading your error log.
Now, after reading all the things above, you are probably thinking: what the heck is that when I just want to start leaning simple SELECT, INSERT, UPDATE, or DELETE statements? Don't worry, here we go:

Selecting Data

So what you are doing in mysql_* is:
<?php
$result = mysql_query('SELECT * from table') or die(mysql_error());

$num_rows = mysql_num_rows($result);

while($row = mysql_fetch_assoc($result)) {
    echo $row['field1'];
}

Now in PDO, you can do this like:
<?php
$stmt = $db->query('SELECT * FROM table');

while($row = $stmt->fetch(PDO::FETCH_ASSOC)) {
    echo $row['field1'];
}

Or
<?php
$stmt = $db->query('SELECT * FROM table');
$results = $stmt->fetchAll(PDO::FETCH_ASSOC);

//Use $results

Note: If you are using the method like below (query()), this method returns a PDOStatement object. So if you want to fetch the result, use it like above.
<?php
foreach($db->query('SELECT * FROM table') as $row) {
    echo $row['field1'];
}

In PDO Data, it is obtained via the ->fetch(), a method of your statement handle. Before calling fetch, the best approach would be telling PDO how you’d like the data to be fetched. In the below section I am explaining this.
Fetch Modes
Note the use of PDO::FETCH_ASSOC in the fetch() and fetchAll() code above. This tells PDO to return the rows as an associative array with the field names as keys. There are many other fetch modes too which I will explain one by one.
First of all, I explain how to select fetch mode:
 $stmt->fetch(PDO::FETCH_ASSOC)

In the above, I have been using fetch(). You can also use:

PDOStatement::fetchAll() - Returns an array containing all of the result set rows
PDOStatement::fetchColumn() - Returns a single column from the next row of a result set
PDOStatement::fetchObject() - Fetches the next row and returns it as an object.
PDOStatement::setFetchMode() - Set the default fetch mode for this statement

Now I come to fetch mode:

PDO::FETCH_ASSOC: returns an array indexed by column name as returned in your result set
PDO::FETCH_BOTH (default): returns an array indexed by both column name and 0-indexed column number as returned in your result set

There are even more choices! Read about them all in PDOStatement Fetch documentation..
Getting the row count:
Instead of using mysql_num_rows to get the number of returned rows, you can get a PDOStatement and do rowCount(), like:
<?php
$stmt = $db->query('SELECT * FROM table');
$row_count = $stmt->rowCount();
echo $row_count.' rows selected';

Getting the Last Inserted ID
<?php
$result = $db->exec(""INSERT INTO table(firstname, lastname) VAULES('John', 'Doe')"");
$insertId = $db->lastInsertId();


Insert and Update or Delete statements

What we are doing in mysql_* function is:
<?php
$results = mysql_query(""UPDATE table SET field='value'"") or die(mysql_error());
echo mysql_affected_rows($result);

And in pdo, this same thing can be done by:
<?php
$affected_rows = $db->exec(""UPDATE table SET field='value'"");
echo $affected_rows;

In the above query PDO::exec execute an SQL statement and returns the number of affected rows.
Insert and delete will be covered later.
The above method is only useful when you are not using variable in query. But when you need to use a variable in a query, do not ever ever try like the above and there for  prepared statement or parameterized statement is.

Prepared Statements
Q. What is a prepared statement and why do I need them?
A. A prepared statement is a pre-compiled SQL statement that can be executed multiple times by sending only the data to the server.
The typical workflow of using a prepared statement is as follows (quoted from Wikipedia three 3 point):

Prepare: The statement template is created by the application and sent to the database management system (DBMS). Certain values are left unspecified, called parameters, placeholders or bind variables (labelled ? below):

`INSERT INTO PRODUCT (name, price) VALUES (?, ?)`


The DBMS parses, compiles, and performs query optimization on the statement template, and stores the result without executing it.
Execute: At a later time, the application supplies (or binds) values for the parameters, and the DBMS executes the statement (possibly returning a result). The application may execute the statement as many times as it wants with different values. In this example, it might supply 'Bread' for the first parameter and 1.00 for the second parameter.

You can use a prepared statement by including placeholders in your SQL. There are basically three ones without placeholders (don't try this with variable its above one), one with unnamed placeholders, and one with named placeholders.
Q. So now, what are named placeholders and how do I use them?
A. Named placeholders. Use descriptive names preceded by a colon, instead of question marks. We don't care about position/order of value in name place holder:
 $stmt->bindParam(':bla', $bla);

bindParam(parameter,variable,data_type,length,driver_options)
You can also bind using an execute array as well:
<?php
$stmt = $db->prepare(""SELECT * FROM table WHERE id=:id AND name=:name"");
$stmt->execute(array(':name' => $name, ':id' => $id));
$rows = $stmt->fetchAll(PDO::FETCH_ASSOC);

Another nice feature for OOP friends is that named placeholders have the ability to insert objects directly into your database, assuming the properties match the named fields. For example:
class person {
    public $name;
    public $add;
    function __construct($a,$b) {
        $this->name = $a;
        $this->add = $b;
    }

}
$demo = new person('john','29 bla district');
$stmt = $db->prepare(""INSERT INTO table (name, add) value (:name, :add)"");
$stmt->execute((array)$demo);

Q. So now, what are unnamed placeholders and how do I use them?
A. Let's have an example:
<?php
$stmt = $db->prepare(""INSERT INTO folks (name, add) values (?, ?)"");
$stmt->bindValue(1, $name, PDO::PARAM_STR);
$stmt->bindValue(2, $add, PDO::PARAM_STR);
$stmt->execute();

and
$stmt = $db->prepare(""INSERT INTO folks (name, add) values (?, ?)"");
$stmt->execute(array('john', '29 bla district'));

In the above, you can see those ? instead of a name like in a name place holder. Now in the first example, we assign variables to the various placeholders ($stmt->bindValue(1, $name, PDO::PARAM_STR);). Then, we assign values to those placeholders and execute the statement. In the second example, the first array element goes to the first ? and the second to the second ?.
NOTE: In unnamed placeholders we must take care of the proper order of the elements in the array that we are passing to the PDOStatement::execute() method.

SELECT, INSERT, UPDATE, DELETE prepared queries

SELECT:
$stmt = $db->prepare(""SELECT * FROM table WHERE id=:id AND name=:name"");
$stmt->execute(array(':name' => $name, ':id' => $id));
$rows = $stmt->fetchAll(PDO::FETCH_ASSOC);

INSERT:
$stmt = $db->prepare(""INSERT INTO table(field1,field2) VALUES(:field1,:field2)"");
$stmt->execute(array(':field1' => $field1, ':field2' => $field2));
$affected_rows = $stmt->rowCount();

DELETE:
$stmt = $db->prepare(""DELETE FROM table WHERE id=:id"");
$stmt->bindValue(':id', $id, PDO::PARAM_STR);
$stmt->execute();
$affected_rows = $stmt->rowCount();

UPDATE:
$stmt = $db->prepare(""UPDATE table SET name=? WHERE id=?"");
$stmt->execute(array($name, $id));
$affected_rows = $stmt->rowCount();



NOTE:
However PDO and/or MySQLi are not completely safe. Check the answer Are PDO prepared statements sufficient to prevent SQL injection? by ircmaxell. Also, I am quoting some part from his answer:
$pdo->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$pdo->query('SET NAMES GBK');
$stmt = $pdo->prepare(""SELECT * FROM test WHERE name = ? LIMIT 1"");
$stmt->execute(array(chr(0xbf) . chr(0x27) . "" OR 1=1 /*""));

"
12859942,12860140,317,"First, let's begin with the standard comment we give everyone: 

Please, don't use mysql_* functions in new code. They are no longer maintained and are officially deprecated. See the red box? Learn about prepared statements instead, and use PDO or MySQLi - this article will help you decide which. If you choose PDO, here is a good tutorial.

Let's go through this, sentence by sentence, and explain:

They are no longer maintained, and are officially deprecated
This means that the PHP community is gradually dropping support for these very old functions. They are likely to not exist in a future (recent) version of PHP! Continued use of these functions may break your code in the (not so) far future.
NEW! - ext/mysql is now officially deprecated as of PHP 5.5!
Newer! ext/mysql has been removed in PHP 7.
Instead, you should learn of prepared statements
mysql_* extension does not support prepared statements, which is (among other things) a very effective countermeasure against SQL Injection. It fixed a very serious vulnerability in MySQL dependent applications which allows attackers to gain access to your script and perform any possible query on your database.
For more information, see How can I prevent SQL injection in PHP?
See the Red Box?
When you go to any mysql function manual page, you see a red box, explaining it should not be used anymore.
Use either PDO or MySQLi
There are better, more robust and well-built alternatives, PDO - PHP Database Object, which offers a complete OOP approach to database interaction, and MySQLi, which is a MySQL specific improvement.

"
12859942,20767765,230,"Ease of use
The analytic and synthetic reasons were already mentioned. For newcomers there's a more significant incentive to stop using the dated mysql_ functions. 
Contemporary database APIs are just easier to use.
It's mostly the bound parameters which can simplify code. And with excellent tutorials (as seen above) the transition to PDO isn't overly arduous.
Rewriting a larger code base at once however takes time. Raison d'être for this intermediate alternative:
Equivalent pdo_* functions in place of mysql_*
Using <pdo_mysql.php> you can switch from the old mysql_ functions with minimal effort. It adds pdo_ function wrappers which replace their mysql_ counterparts.

Simply include_once(""pdo_mysql.php""); in each invocation script that has to interact with the database.

Remove the mysql_ function prefix everywhere and replace it with pdo_.

mysql_connect() becomes pdo_connect()
mysql_query() becomes pdo_query()
mysql_num_rows() becomes pdo_num_rows()
mysql_insert_id() becomes pdo_insert_id()
mysql_fetch_array() becomes pdo_fetch_array()
mysql_fetch_assoc() becomes pdo_fetch_assoc()
mysql_real_escape_string() becomes pdo_real_escape_string()
and so on...  

Your code will work alike and still mostly look the same:
include_once(""pdo_mysql.php""); 

pdo_connect(""localhost"", ""usrABC"", ""pw1234567"");
pdo_select_db(""test"");

$result = pdo_query(""SELECT title, html FROM pages"");  

while ($row = pdo_fetch_assoc($result)) {
    print ""$row[title] - $row[html]"";
}


Et voilà.
Your code is using PDO.
Now it's time to actually utilize it.  
Bound parameters can be easy to use

You just need a less unwieldy API.
pdo_query() adds very facile support for bound parameters. Converting old code is straightforward:

Move your variables out of the SQL string.

Add them as comma delimited function parameters to pdo_query().
Place question marks ? as placeholders where the variables were before.
Get rid of ' single quotes that previously enclosed string values/variables.

The advantage becomes more obvious for lengthier code.
Often string variables aren't just interpolated into SQL, but concatenated with escaping calls in between.
pdo_query(""SELECT id, links, html, title, user, date FROM articles
   WHERE title='"" . pdo_real_escape_string($title) . ""' OR id='"".
   pdo_real_escape_string($title) . ""' AND user <> '"" .
   pdo_real_escape_string($root) . ""' ORDER BY date"")

With ? placeholders applied you don't have to bother with that:
pdo_query(""SELECT id, links, html, title, user, date FROM articles
   WHERE title=? OR id=? AND user<>? ORDER BY date"", $title, $id, $root)

Remember that pdo_* still allows either or.
Just don't escape a variable and bind it in the same query.

The placeholder feature is provided by the real PDO behind it.
Thus also allowed :named placeholder lists later.

More importantly you can pass $_REQUEST[] variables safely behind any query. When submitted <form> fields match the database structure exactly it's even shorter:
pdo_query(""INSERT INTO pages VALUES (?,?,?,?,?)"", $_POST);

So much simplicity. But let's get back to some more rewriting advises and technical reasons on why you may want to get rid of mysql_ and escaping.
Fix or remove any oldschool sanitize() function
Once you have converted all mysql_ calls to pdo_query with bound params, remove all redundant pdo_real_escape_string calls.
In particular you should fix any sanitize or clean or filterThis or clean_data functions as advertised by dated tutorials in one form or the other:
function sanitize($str) {
   return trim(strip_tags(htmlentities(pdo_real_escape_string($str))));
}

Most glaring bug here is the lack of documentation. More significantly the order of filtering was in exactly the wrong order.

Correct order would have been: deprecatedly stripslashes as the innermost call, then trim, afterwards strip_tags, htmlentities for output context, and only lastly the _escape_string as its application should directly preceed the SQL intersparsing.
But as first step just get rid of the _real_escape_string call.
You may have to keep the rest of your sanitize() function for now if your database and application flow expect HTML-context-safe strings. Add a comment that it applies only HTML escaping henceforth.
String/value handling is delegated to PDO and its parameterized statements.
If there was any mention of stripslashes() in your sanitize function, it may indicate a higher level oversight.

That was commonly there to undo damage (double escaping) from the deprecated magic_quotes. Which however is best fixed centrally, not string by string.
Use one of the userland reversal approaches. Then remove the stripslashes() in the sanitize function.


Historic note on magic_quotes. That feature is rightly deprecated. It's often incorrectly portrayed as failed security feature however. But magic_quotes are as much a failed security feature as tennis balls have failed as nutrition source. That simply wasn't their purpose.
The original implementation in PHP2/FI introduced it explicitly with just ""quotes will be automatically escaped making it easier to pass form data directly to msql queries"". Notably it was accidentially safe to use with mSQL, as that supported ASCII only.
  Then PHP3/Zend reintroduced magic_quotes for MySQL and misdocumented it. But originally it was just a convenience feature, not intend for security. 


How prepared statements differ
When you scramble string variables into the SQL queries, it doesn't just get more intricate for you to follow. It's also extraneous effort for MySQL to segregate code and data again.

SQL injections simply are when data bleeds into code context. A database server can't later spot where PHP originally glued variables inbetween query clauses.
With bound parameters you separate SQL code and SQL-context values in your PHP code. But it doesn't get shuffled up again behind the scenes (except with PDO::EMULATE_PREPARES). Your database receives the unvaried SQL commands and 1:1 variable values.

While this answer stresses that you should care about the readability advantages of dropping mysql_. There's occasionally also a performance advantage (repeated INSERTs with just differing values) due to this visible and technical data/code separation. 
Beware that parameter binding still isn't a magic one-stop solution against all SQL injections. It handles the most common use for data/values. But can't whitelist column name / table identifiers, help with dynamic clause construction, or just plain array value lists.
Hybrid PDO use
These pdo_* wrapper functions make a coding-friendly stop-gap API. (It's pretty much what MYSQLI could have been if it wasn't for the idiosyncratic function signature shift). They also expose the real PDO at most times.
Rewriting doesn't have to stop at using the new pdo_ function names. You could one by one transition each pdo_query() into a plain $pdo->prepare()->execute() call.
It's best to start at simplifying again however. For example the common result fetching:
$result = pdo_query(""SELECT * FROM tbl"");
while ($row = pdo_fetch_assoc($result)) {

Can be replaced with just an foreach iteration:
foreach ($result as $row) {

Or better yet a direct and complete array retrieval:
$result->fetchAll();

You'll get more helpful warnings in most cases than PDO or mysql_ usually provide after failed queries.
Other options
So this hopefully visualized some practical reasons and a worthwile pathway to drop mysql_.
Just switching to pdo doesn't quite cut it. pdo_query() is also just a frontend onto it.
Unless you also introduce parameter binding or can utilize something else from the nicer API, it's a pointless switch. I hope it's portrayed simple enough to not further the discouragement to newcomers. (Education usually works better than prohibition.)
While it qualifies for the simplest-thing-that-could-possibly-work category, it's also still very experimental code. I just wrote it over the weekend. There's a plethora of alternatives however. Just google for PHP database abstraction and browse a little. There always have been and will be lots of excellent libraries for such tasks.
If you want to simplify your database interaction further, mappers like Paris/Idiorm are worth a try. Just like nobody uses the bland DOM in JavaScript anymore, you don't have to babysit a raw database interface nowadays.
"
12859942,12860030,160,"The mysql_ functions:

are out of date - they're not maintained any more
don't allow you to move easily to another database backend
don't support prepared statements, hence
encourage programmers to use concatenation to build queries, leading to SQL injection vulnerabilities

"
